{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953226f9",
   "metadata": {},
   "source": [
    "# First version of AE with clustering loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f30b30",
   "metadata": {},
   "source": [
    "## Import useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2d3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scanpy in /shared/home/tberthet/.local/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: anndata>=0.8 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.10.7)\n",
      "Requirement already satisfied: h5py>=3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (3.11.0)\n",
      "Requirement already satisfied: joblib in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.4.2)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (3.8.4)\n",
      "Requirement already satisfied: natsort in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: networkx>=2.7 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (3.3)\n",
      "Requirement already satisfied: numba>=0.56 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.59.1)\n",
      "Requirement already satisfied: numpy>=1.23 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scanpy) (1.26.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scanpy) (23.2)\n",
      "Requirement already satisfied: pandas>=1.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scanpy) (2.2.0)\n",
      "Requirement already satisfied: patsy in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.5.6)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.5.12)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.8 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.13.0)\n",
      "Requirement already satisfied: seaborn>=0.13 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.13.2)\n",
      "Requirement already satisfied: session-info in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.14.2)\n",
      "Requirement already satisfied: tqdm in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (4.66.4)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.5.6)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from anndata>=0.8->scanpy) (1.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from numba>=0.56->scanpy) (0.42.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas>=1.5->scanpy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas>=1.5->scanpy) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
      "Requirement already satisfied: six in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from patsy->scanpy) (1.16.0)\n",
      "Requirement already satisfied: stdlib-list in /shared/home/tberthet/.local/lib/python3.12/site-packages (from session-info->scanpy) (0.10.0)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/bin/pip\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/cli/main.py\", line 79, in main\n",
      "    return command.main(cmd_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 236, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/cli/req_command.py\", line 188, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/self_outdated_check.py\", line 231, in pip_self_version_check\n",
      "    installed_dist = get_default_environment().get_distribution(\"pip\")\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 189, in get_distribution\n",
      "    return next(matches, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 184, in <genexpr>\n",
      "    matches = (\n",
      "              ^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/metadata/base.py\", line 626, in iter_all_distributions\n",
      "    for dist in self._iter_distributions():\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 176, in _iter_distributions\n",
      "    yield from finder.find(location)\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 79, in find\n",
      "    for dist, info_location in self._find_impl(location):\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/metadata/importlib/_envs.py\", line 64, in _find_impl\n",
      "    raw_name = get_dist_name(dist)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages/pip/_internal/metadata/importlib/_compat.py\", line 52, in get_dist_name\n",
      "    name = cast(Any, dist).name\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/importlib/metadata/__init__.py\", line 457, in name\n",
      "    return self.metadata['Name']\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/importlib/metadata/__init__.py\", line 444, in metadata\n",
      "    self.read_text('METADATA')\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/importlib/metadata/__init__.py\", line 818, in read_text\n",
      "    return self._path.joinpath(filename).read_text(encoding='utf-8')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/pathlib.py\", line 1027, in read_text\n",
      "    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/pathlib.py\", line 1013, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 309, in __init__\n",
      "KeyboardInterrupt\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /shared/home/tberthet/.local/lib/python3.12/site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from optree->keras) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from rich->keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /shared/home/tberthet/.local/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (69.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: rich in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /shared/home/tberthet/.local/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: igraph in /shared/home/tberthet/.local/lib/python3.12/site-packages (0.11.5)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from igraph) (1.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: leidenalg in /shared/home/tberthet/.local/lib/python3.12/site-packages (0.10.2)\n",
      "Requirement already satisfied: igraph<0.12,>=0.10.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from leidenalg) (0.11.5)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from igraph<0.12,>=0.10.0->leidenalg) (1.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for csv\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scanpy\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip3 install igraph\n",
    "!pip3 install leidenalg\n",
    "!pip install os\n",
    "!pip install csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ae1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 19:37:14.899928: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 19:37:16.110102: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 19:37:17.390476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Layer\n",
    "from keras.models import load_model, Model\n",
    "from keras import backend as K\n",
    "from keras.losses import KLDivergence\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, accuracy_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import seaborn as sns\n",
    "from layers import ConstantDispersionLayer, SliceLayer, ColWiseMultLayer\n",
    "import keras\n",
    "from keras.layers import Layer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9baf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c088e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8432bf",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaf695",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287976f",
   "metadata": {},
   "source": [
    "On importe le dataset baron et on applique le prétraitement habituel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b855d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./dataset/barondata_rawcounts.csv\"\n",
    "data_baron=pd.read_csv(filename, sep=';', comment=\"#\", index_col=0)\n",
    "data_baron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e162be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./dataset/barondata_annotations.csv\"\n",
    "annotations_baron=pd.read_csv(filename, sep=';', comment=\"#\", index_col=0)\n",
    "annotations_baron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann=sc.AnnData(data_baron.T)\n",
    "data_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8599d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.obs['label'] = annotations_baron['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98b9b4",
   "metadata": {},
   "source": [
    "### Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8b136",
   "metadata": {},
   "source": [
    "On filtre les données de manière peu stricte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(data_ann, min_genes=1)\n",
    "sc.pp.filter_genes(data_ann, min_cells=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.raw = data_ann.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d30f5",
   "metadata": {},
   "source": [
    "### Normalize and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219882fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(data_ann)\n",
    "data_ann.obs['size_factors'] = data_ann.obs.n_genes / np.median(data_ann.obs.n_genes)\n",
    "#data_ann.obs['size_factors'] = 1.0\n",
    "sc.pp.log1p(data_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(data_ann, n_top_genes=2000, batch_key=\"label\")\n",
    "sc.pl.highly_variable_genes(data_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490890e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.scale(data_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb6129",
   "metadata": {},
   "source": [
    "### Explore the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b31b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930af5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.n_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744468b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2814ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.raw.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e453889",
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_variable_genes = data_ann.var[data_ann.var['highly_variable']].index.tolist()\n",
    "count_data_hvg = data_ann[:, highly_variable_genes].X\n",
    "count_data_hvg=count_data_hvg.toarray()\n",
    "count_data_hvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3942d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_count_hvg=data_ann.raw[:,highly_variable_genes].X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0438fa-6783-4551-bb19-df719081a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_count_hvg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb7e32",
   "metadata": {},
   "source": [
    "## Create autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_binomial_loss(y_true, y_pred):\n",
    "    input_shape=int(y_pred.shape[1]/3)\n",
    "    mu = y_pred[:, :input_shape]\n",
    "    pi = y_pred[:, input_shape:input_shape*2]\n",
    "    theta = y_pred[:, input_shape*2:]\n",
    "    y_true = tf.cast(y_true, dtype='float32')\n",
    "    #print(type(mu), mu)\n",
    "    #print(type(pi),pi)\n",
    "    #print(type(theta), theta)\n",
    "    #print(type(y_true), y_true)\n",
    "\n",
    "\n",
    "    eps = 1e-10\n",
    "    t1 = tf.math.lgamma(theta+eps) + tf.math.lgamma(y_true+1.0) - tf.math.lgamma(y_true+theta+eps)\n",
    "    t2 = (theta+y_true) * tf.math.log(1.0 + (mu/(theta+eps))) + (y_true * (tf.math.log(theta+eps) - tf.math.log(mu+eps)))\n",
    "    final=t1 + t2\n",
    "    final = tf.reduce_mean(final)\n",
    "    return final\n",
    "\n",
    "def zero_inflated_negative_binomial_loss(y_true, y_pred):\n",
    "    input_shape=int(y_pred.shape[1]/3)\n",
    "    mu = y_pred[:, :input_shape]\n",
    "    pi = y_pred[:, input_shape:input_shape*2]\n",
    "    theta = y_pred[:, input_shape*2:]\n",
    "    y_true = tf.cast(y_true, dtype='float32')\n",
    "    #print(type(mu), mu)\n",
    "    #print(type(pi),pi)\n",
    "    #print(type(theta), theta)\n",
    "    #print(type(y_true), y_true)\n",
    "\n",
    "\n",
    "    eps = 1e-10\n",
    "    t1 = tf.math.lgamma(theta+eps) + tf.math.lgamma(y_true+1.0) - tf.math.lgamma(y_true+theta+eps)\n",
    "    t2 = (theta+y_true) * tf.math.log(1.0 + (mu/(theta+eps))) + (y_true * (tf.math.log(theta+eps) - tf.math.log(mu+eps)))\n",
    "    final=t1 + t2\n",
    "\n",
    "    nb_case = t1 + t2 - tf.math.log(1.0-pi+eps)\n",
    "    zero_nb = tf.pow(theta/(theta+mu+eps), theta)\n",
    "    zero_case = -tf.math.log(pi + ((1.0-pi)*zero_nb)+eps)\n",
    "    result = tf.where(tf.less(y_true, 1e-8), zero_case, nb_case)\n",
    "    #ridge = self.ridge_lambda*tf.square(self.pi)\n",
    "    #result += ridge\n",
    "    result = tf.reduce_mean(result)\n",
    "    return result\n",
    "\n",
    "MeanAct = lambda x: tf.clip_by_value(tf.keras.backend.exp(x), 1e-5, 1e6)\n",
    "DispAct = lambda x: tf.clip_by_value(tf.keras.backend.softplus(x), 1e-4, 1e4)\n",
    "\n",
    "ColWiseMultLayer = lambda name: layers.Lambda(lambda l: l[0]*(tf.matmul(tf.reshape(l[1], (-1,1)),\n",
    "                                                                 tf.ones((1, l[0].get_shape()[1]),\n",
    "                                                                         dtype=l[1].dtype))), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters=n_clusters\n",
    "        self.alpha=alpha\n",
    "        self.intial_weights=weights\n",
    "        #self.input_spec=keras.InputSpec(ndim=2) #to specify the expected rank of the input\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim=input_shape[1]\n",
    "        #self.input_spec=keras.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform')\n",
    "        if self.intial_weights is not None :\n",
    "            self.set_weights(self.intial_weights)\n",
    "            del self.intial_weights\n",
    "        self.built=True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        q = 1.0 / (1.0 + (tf.math.reduce_sum(tf.math.square(tf.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = tf.transpose(tf.transpose(q) / tf.math.reduce_sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_shape, noise):\n",
    "    init='glorot_uniform'\n",
    "    Inputs = layers.Input(shape=(input_shape,), name='Inputs')\n",
    "    sf_layer=layers.Input(shape=(1,), name=\"size_factors\")\n",
    "    x=layers.GaussianNoise(noise)(Inputs)\n",
    "    x=layers.Dense(256, activation='relu',kernel_initializer=init, name='encoder_1' )(x)\n",
    "    x=layers.GaussianNoise(noise)(x)\n",
    "    x=layers.Dense(64, activation='relu',kernel_initializer=init, name='encoder_2' )(x)\n",
    "    x=layers.GaussianNoise(noise)(x)\n",
    "    hidden=layers.Dense(32, activation='relu',kernel_initializer=init, name='encoder_3' )(x)\n",
    "\n",
    "\n",
    "    x=layers.Dense(32, activation='relu',kernel_initializer=init, name='decoder_1' )(hidden)\n",
    "    x=layers.Dense(64, activation='relu',kernel_initializer=init, name='decoder_2' )(x)\n",
    "    x=layers.Dense(256, activation='relu',kernel_initializer=init, name='decoder_3' )(x)\n",
    "    pi=layers.Dense(input_shape, activation=\"sigmoid\",kernel_initializer=init, name='pi')(x)\n",
    "    disp=layers.Dense(input_shape, activation=DispAct,kernel_initializer=init, name='dispersion')(x)\n",
    "    mean=layers.Dense(input_shape, activation=MeanAct,kernel_initializer=init, name='mean')(x)\n",
    "\n",
    "    Outputs=ColWiseMultLayer(name='outputs')([mean, sf_layer])\n",
    "    #Outputs=SliceLayer(0, name='slice')([Outputs, disp, pi])\n",
    "    outputs = layers.Concatenate(axis=1, name='output')([Outputs, pi, disp])\n",
    "\n",
    "    autoencoder=Model([Inputs, sf_layer], outputs, name='autoencoder_ZINB')\n",
    "    autoencoder.compile(optimizer='adam', loss={'output': zero_inflated_negative_binomial_loss})\n",
    "\n",
    "    autoencoder.summary()\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=data_ann.n_vars\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93003b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_ann.X)\n",
    "data_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4723f50",
   "metadata": {},
   "source": [
    "On entraîne le modèle sur l'ensemble des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_latent_space(y_pred, model, count_data_hvg , size_factors, obs):\n",
    "    encoder= Model(inputs=model.input, outputs=model.get_layer(\"encoder_3\").output)\n",
    "    encoder.summary()\n",
    "    predict_data=encoder.predict([count_data_hvg, size_factors])\n",
    "    adata_latent = sc.AnnData(predict_data)\n",
    "    adata_latent.obs=obs\n",
    "    adata_latent.obs['predict']=y_pred\n",
    "    anno=adata_latent.obs['label']\n",
    "    sc.pp.neighbors(adata_latent)\n",
    "    sc.tl.umap(adata_latent)\n",
    "    sc.pl.umap(adata_latent, color=\"label\")\n",
    "    sc.pl.umap(adata_latent, color=\"predict\")\n",
    "    crosstab = pd.crosstab(y_pred,anno)\n",
    "    sns.heatmap(crosstab, annot=True, cmap='Blues')\n",
    "    plt.ylabel('Clusters prédits')\n",
    "    plt.xlabel('Annotations réelles')\n",
    "    plt.title('Matrice de confusion')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe54b7-e60e-42da-b27b-33c757a87355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "   \n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    # Assurez-vous que les étiquettes sont de type str\n",
    "    y_true= y_true.astype(str)\n",
    "    y_pred = y_pred.astype(str)\n",
    "    \n",
    "    # Trouver les étiquettes uniques\n",
    "    labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    # Construire la matrice de coût (matrice de confusion)\n",
    "    cost_matrix = np.zeros((n_labels, n_labels), dtype=int)\n",
    "    for i, label_true in enumerate(labels):\n",
    "        for j, label_pred in enumerate(labels):\n",
    "            cost_matrix[i, j] = np.sum((y_true == label_true) & (y_pred == label_pred))\n",
    "\n",
    "    # Résoudre le problème de correspondance bipartite optimal\n",
    "    row_ind, col_ind = linear_assignment(cost_matrix.max() - cost_matrix)\n",
    "\n",
    "    # Calculer la précision\n",
    "    accuracy = np.sum([cost_matrix[i, j] for i, j in zip(row_ind, col_ind)]) / y_true.size\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cced50e-ebb3-47e8-bec1-cde59122b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(history, filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, mode='a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(history.keys())\n",
    "        writer.writerow(history.values())\n",
    "        \n",
    "def check_existing_filename(filename):\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    while os.path.exists(filename):\n",
    "        filename = f\"{base}_{counter}{ext}\"\n",
    "        counter += 1\n",
    "    return filename\n",
    "\n",
    "def save_plot_umap(model, x, size_factors, y, y_pred, res, iteration, pdf_pages, train_test=\"train\"):\n",
    "    #Récupération des données et projection dans l'espace latent \n",
    "    encoder= Model(inputs=model.input, outputs=model.get_layer(\"encoder_3\").output)\n",
    "    predict_data=encoder.predict([x, size_factors], verbose=0)\n",
    "    obs_df = pd.DataFrame({'label': y})\n",
    "    \n",
    "    #Préparation des données pour Scanpy\n",
    "    adata_latent = sc.AnnData(X=predict_data)\n",
    "    adata_latent.obs = obs_df\n",
    "    adata_latent.obs['predict'] = y_pred.astype(str)\n",
    "\n",
    "    sc.pp.neighbors(adata_latent)\n",
    "    sc.tl.umap(adata_latent)\n",
    "    \n",
    "    # Génération du UMAP avec Scanpy\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    sc.pl.umap(adata_latent, color='label', ax=axs[0], show=False)\n",
    "    axs[0].set_title(f'UMAP projection - Labels ({train_test}) (Res: {res}, Iter: {iteration})')\n",
    "    sc.pl.umap(adata_latent, color='predict', ax=axs[1], show=False)\n",
    "    axs[1].set_title(f'UMAP projection - Predictions ({train_test}) (Res: {res}, Iter: {iteration})')\n",
    "    \n",
    "\n",
    "    pdf_pages.savefig(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39a540-af78-44eb-bb8c-1e0ad6e20b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.square(y_true - y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19122365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q**2/q.sum(0)\n",
    "    return (weight.T/weight.sum(1)).T\n",
    "\n",
    "def auto_kmeans(encoder, x_counts, size_factors, obs, n_cluster='auto', plot=False):\n",
    "    y=obs\n",
    "    ari=[]\n",
    "    nmi=[]\n",
    "    ca=[]\n",
    "    x=[]\n",
    "    if n_cluster=='auto':\n",
    "        for n in range (1,20):\n",
    "            kmeans=KMeans(n_clusters=n, n_init=30, verbose=0)\n",
    "            y_pred=kmeans.fit_predict(encoder.predict([x_counts, size_factors]))\n",
    "            ari.append(adjusted_rand_score(y, y_pred))\n",
    "            nmi.append(normalized_mutual_info_score(y, y_pred))\n",
    "            ca.append(cluster_acc(y, y_pred))\n",
    "            x.append(n)\n",
    "        somme_metriques = [x + y + z for x, y, z in zip(ari, nmi, ca)]\n",
    "        n_cluster=(somme_metriques.index(max(somme_metriques))+1)\n",
    "    \n",
    "    kmeans=KMeans(n_clusters=n_cluster, n_init=20)\n",
    "    y_pred=kmeans.fit_predict(encoder.predict([x_counts, size_factors]))\n",
    "    \n",
    "    if plot==True:\n",
    "        predict_data=encoder.predict([x_counts, size_factors])\n",
    "        adata_latent = sc.AnnData(predict_data)\n",
    "        obs_df = pd.DataFrame({'label': y})\n",
    "        adata_latent.obs=obs_df\n",
    "        adata_latent.obs[\"kmeans\"]=y_pred\n",
    "        sc.pp.neighbors(adata_latent)\n",
    "        sc.tl.umap(adata_latent)\n",
    "        sc.pl.umap(adata_latent, color='label')\n",
    "        sc.pl.umap( adata_latent, color=[\"kmeans\"])\n",
    "        plt.plot(x,ari)\n",
    "        plt.plot(x,nmi)\n",
    "        plt.plot(x,ca)\n",
    "        plt.xlabel(\"nombre de clusters\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.legend(labels=[\"ARI\", \"NMI\", \"CA\"])\n",
    "        plt.plot()\n",
    "        ari = adjusted_rand_score(y, y_pred)\n",
    "        print(\"Indice de Rand ajusté (ARI) :\", ari)\n",
    "        nmi = normalized_mutual_info_score(y, y_pred)\n",
    "        print(\"Normalized mutual info (NMI) :\", nmi)\n",
    "        ca=cluster_acc(y, y_pred)\n",
    "        print(\"Clustering accuracy (CA) :\", ca)\n",
    "    return y_pred, n_cluster, kmeans.cluster_centers_\n",
    "\n",
    "def auto_leiden(encoder, x_counts, size_factors, y, res=\"auto\", plot=False):\n",
    "    predict_data=encoder.predict([x_counts, size_factors], verbose=0)\n",
    "    adata_latent = sc.AnnData(predict_data)\n",
    "    obs_df = pd.DataFrame({'label': y})\n",
    "    adata_latent.obs=obs_df\n",
    "    sc.pp.neighbors(adata_latent)\n",
    "    sc.tl.umap(adata_latent)\n",
    "    list_ari=[]\n",
    "    list_nmi=[]\n",
    "    x=[]\n",
    "    \n",
    "    if res==\"auto\":\n",
    "        #search for the best resolution\n",
    "        for i in range (1,10):\n",
    "            sc.tl.leiden(adata_latent, key_added=\"leiden\", resolution=i/100)\n",
    "            predict_cluster=adata_latent.obs[\"leiden\"]\n",
    "            list_ari.append(adjusted_rand_score(y, predict_cluster))\n",
    "            list_nmi.append(normalized_mutual_info_score(y, predict_cluster))\n",
    "            x.append(i/100)\n",
    "        for i in range (1,11):\n",
    "            sc.tl.leiden(adata_latent, key_added=\"leiden\", resolution=i/10)\n",
    "            predict_cluster=adata_latent.obs[\"leiden\"]\n",
    "            list_ari.append(adjusted_rand_score(y, predict_cluster))\n",
    "            list_nmi.append(normalized_mutual_info_score(y, predict_cluster))\n",
    "            x.append(i/10)\n",
    "        somme_metriques = [x + y for x, y in zip(list_ari, list_nmi)]\n",
    "        res=x[somme_metriques.index(max(somme_metriques))]\n",
    "        print(\"La résolution est de : \", res)\n",
    "    #compute for the best resolution\n",
    "    sc.tl.leiden(adata_latent, key_added=\"leiden_res_%.4f\" % (res), resolution=res)\n",
    "    predict=adata_latent.obs[\"leiden_res_%.4f\" % (res)]\n",
    "    \n",
    "    \n",
    "    #compute cluster center for initialization\n",
    "    init_pred=np.asarray(predict,dtype=int)\n",
    "    features=pd.DataFrame(adata_latent.X,index=np.arange(0,adata_latent.shape[0]))\n",
    "    Group=pd.Series(init_pred,index=np.arange(0,adata_latent.shape[0]),name=\"Group\")\n",
    "    Mergefeature=pd.concat([features,Group],axis=1)\n",
    "    cluster_centers=np.asarray(Mergefeature.groupby(\"Group\").mean())\n",
    "    n_clusters=len(np.unique(init_pred))\n",
    "    \n",
    "    #set of plot if required\n",
    "    if plot==True:\n",
    "        sc.pl.umap(adata_latent, color='label')\n",
    "        plt.plot(x,list_ari)\n",
    "        plt.xlabel(\"nombre de clusters\")\n",
    "        plt.ylabel(\"ARI\")\n",
    "        plt.plot(x,list_nmi)\n",
    "        plt.xlabel(\"nombre de clusters\")\n",
    "        plt.ylabel(\"NMI\")\n",
    "        sc.pl.umap( adata_latent, color=[\"leiden_res_%.4f\" % (res)], legend_loc=\"on data\")\n",
    "        ari = adjusted_rand_score(y, predict)\n",
    "        print(\"Indice de Rand ajusté (ARI) :\", ari)\n",
    "        nmi = normalized_mutual_info_score(y, predict)\n",
    "        print(\"Normalized mutual info (NMI) :\", nmi)\n",
    "        plt.plot()\n",
    "        #crosstab = pd.crosstab(predict,y)\n",
    "        #sns.heatmap(crosstab, annot=True, cmap='Blues')\n",
    "        #plt.ylabel('Clusters prédits')\n",
    "        #plt.xlabel('Annotations réelles')\n",
    "        #plt.title('Matrice de confusion')\n",
    "        #plt.show()\n",
    "    return res, predict, n_clusters, cluster_centers\n",
    "    \n",
    "def split(x_counts, raw_counts, size_factors, y):\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(y)), stratify=y, test_size=0.2, random_state=42)\n",
    "    x_train=x_counts[train_idx]\n",
    "    x_test=x_counts[test_idx]\n",
    "    size_factors_train=size_factors[train_idx]\n",
    "    size_factors_test=size_factors[test_idx]\n",
    "    raw_train=raw_counts[train_idx]\n",
    "    raw_test=raw_counts[test_idx]\n",
    "    y_train=y[train_idx]\n",
    "    y_test=y[test_idx]\n",
    "    print(\"Size of train set : \", x_train.shape)\n",
    "    print(\"Size of test set : \", x_test.shape)\n",
    "    return x_train, x_test, size_factors_train, size_factors_test, raw_train, raw_test, y_train, y_test\n",
    "    \n",
    "def fit_and_split(x_counts, obs, size_factors, raw_counts, alpha, n_cluster=\"auto\", res=\"auto\", method=\"leiden\", noise=0.5, batch_size=256,\n",
    "        max_iter=2e4, tol=1e-3, update_interval=140, loss_weights=[1.0,1.0,1.0],\n",
    "        ae_weights=None, pretrained=False):\n",
    "    print('Update interval', update_interval)\n",
    "    t0 = time.time()\n",
    "    y=obs[\"label\"]\n",
    "    #step 0 split data\n",
    "    x_train, x_test, size_factors_train, size_factors_test, raw_train, raw_test, y_train, y_test=split(x_counts, raw_counts, size_factors, y)\n",
    "    input_shape=x_train.shape[1]\n",
    "    #Step 1 Pretrain \n",
    "    if pretrained==False or ae_weights is None :\n",
    "        print(\"..pretraining autoencoder : \")\n",
    "        autoencoder=create_autoencoder(input_shape, noise)\n",
    "        callback= tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, mode='min', verbose=1)\n",
    "        autoencoder.fit(x=[x_train, size_factors_train], y=raw_train, validation_data=([x_test,size_factors_test], raw_test), batch_size=batch_size, epochs=200, callbacks=[callback])\n",
    "        autoencoder.save_weights(\"./model/weights/scziDesk.weights.h5\")\n",
    "        ae_weights=\"./model/weights/dif_ae_res.weights.h5\"\n",
    "    elif ae_weights is not None:\n",
    "            autoencoder=create_autoencoder(input_shape, noise)\n",
    "            autoencoder.load_weights(ae_weights)\n",
    "            print('ae_weights is loaded successfully.')\n",
    "    \n",
    "    ae_layers = [l for l in autoencoder.layers]\n",
    "    hidden = autoencoder.input[0]\n",
    "    for i in range(1, len(ae_layers)):\n",
    "        if \"noise\" in ae_layers[i].name:\n",
    "            next\n",
    "        elif \"dropout\" in ae_layers[i].name:\n",
    "            next\n",
    "        else:\n",
    "            hidden = ae_layers[i](hidden)\n",
    "        if \"encoder_3\" in ae_layers[i].name:  # only get encoder layers\n",
    "             break\n",
    "    encoder = Model(inputs=autoencoder.input, outputs=hidden, name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    \n",
    "    #step 2 intialize clusters:\n",
    "    \n",
    "    #récupérons l'autoencoder\n",
    "    #encoder= Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(\"hidden\").output)\n",
    "    if method==\"leiden\":\n",
    "        print(\"Initializing cluster centers with leiden : \")\n",
    "        res, y_pred, n_cluster, cluster_centers = auto_leiden(encoder, x_train, size_factors_train, y_train, res=res, plot=False)\n",
    "        print('Le nombre de clusters est : ', n_cluster)\n",
    "    elif method==\"kmeans\":\n",
    "        print(\"Initializing cluster centers with k-means : \")\n",
    "        y_pred, n_cluster, cluster_centers=auto_kmeans(encoder, x_train, size_factors_train, y_train, n_cluster=n_cluster, plot=False)\n",
    "        print('Le nombre de clusters est : ', n_cluster)\n",
    "        \n",
    "    y_pred_last_train=np.copy(y_pred)\n",
    "    \n",
    "    clustering_layer = ClusteringLayer(n_cluster, alpha=alpha, name='clustering')(hidden)\n",
    "    model= Model(inputs=[autoencoder.input[0], autoencoder.input[1]],\n",
    "                           outputs=[clustering_layer, autoencoder.output, hidden])\n",
    "    model.summary()\n",
    "    model.compile(loss={'clustering': KLDivergence, 'output': zero_inflated_negative_binomial_loss, 'kmeans': kmeans_loss}, optimizer='adam', metrics={'clustering': KLDivergence, 'output': zero_inflated_negative_binomial_loss, 'kmeans':kmeans_loss})\n",
    "    print(\"Set clustering weights\")\n",
    "    model.get_layer(name='clustering').set_weights([cluster_centers])\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #step 3 deep clustering\n",
    "    print(\"..Starting Deep Clustering\")\n",
    "    loss=[0,0,0,0]\n",
    "    val_loss=[0,0,0,0]\n",
    "    index=0\n",
    "    save_interval = int(x_counts.shape[0] / batch_size) * 5 \n",
    "    \n",
    "    #Création du dictionnaire pour le monitoring\n",
    "    history={\n",
    "        \"res\":res,\n",
    "        \"clusters\": n_cluster,\n",
    "        \"weight\": loss_weights[0],\n",
    "        \"NMI\" :[],\n",
    "        \"ARI\" :[],\n",
    "        \"CA\" :[],\n",
    "        \"val_NMI\":[],\n",
    "        \"val_ARI\":[],\n",
    "        \"val_CA\":[],\n",
    "        \"loss\":[],\n",
    "        \"val_loss\":[],\n",
    "        \"clustering_loss\":[],\n",
    "        \"val_clustering_loss\":[],\n",
    "        \"zinb_loss\":[],\n",
    "        \"val_zinb_loss\":[],\n",
    "        \"kmeans_loss\":[],\n",
    "        \"val_kmeans_loss\":[]\n",
    "    }\n",
    "        \n",
    "    #initiate pdf files\n",
    "    pdf_filename_train = 'data/scziDesk/plots/plots_train_clusters_%.4f.pdf' % n_cluster\n",
    "    #pdf_filename_train = check_existing_filename(pdf_filename_train)\n",
    "    pdf_pages_train = PdfPages(pdf_filename_train)\n",
    "    \n",
    "    pdf_filename_test = 'data/scziDesk//plots/plots_test_clusters_%.4f.pdf' % n_cluster\n",
    "    #pdf_filename_test = check_existing_filename(pdf_filename_test)\n",
    "    pdf_pages_test = PdfPages(pdf_filename_test)\n",
    "    \n",
    "    for iteration in range(int(max_iter)):\n",
    "       \n",
    "        #if iteration % update_interval==0:\n",
    "        \n",
    "        if index==0:\n",
    "            q_train,_= model.predict([x_train, size_factors_train], verbose=0)\n",
    "            p_train=target_distribution(q_train)\n",
    "            q_test,_= model.predict([x_test, size_factors_test], verbose=0)\n",
    "            p_test=target_distribution(q_test)\n",
    "            \n",
    "            loss=model.evaluate(x=[x_train, size_factors_train], y=[p_train, raw_train, cluster_centers], batch_size=batch_size, verbose=0)\n",
    "            val_loss=model.evaluate(x=[x_test, size_factors_test], y=[p_test, raw_test, cluster_centers], batch_size=batch_size, verbose=0)\n",
    "            \n",
    "            y_pred_train=q_train.argmax(1)\n",
    "            y_pred_test=q_test.argmax(1)\n",
    "            if y is not None :\n",
    "                ca=np.round(cluster_acc(y_train, y_pred_train), 5)\n",
    "                nmi=np.round(normalized_mutual_info_score(y_train, y_pred_train), 5)\n",
    "                ari=np.round(adjusted_rand_score(y_train, y_pred_train), 5)\n",
    "                val_ca=np.round(cluster_acc(y_test, y_pred_test), 5)\n",
    "                val_nmi=np.round(normalized_mutual_info_score(y_test, y_pred_test), 5)\n",
    "                val_ari=np.round(adjusted_rand_score(y_test, y_pred_test), 5)\n",
    "                print('Iter-%d: CA=%.4f, NMI= %.4f, ARI= %.4f; L= %.5f, Lc= %.5f,  Lr= %.5f'\n",
    "                          % (iteration, ca, nmi, ari, loss[0], loss[1], loss[2]))\n",
    "                print('CA=%.4f, val_NMI= %.4f, val_ARI= %.4f; val_L= %.5f, val_Lc= %.5f,  val_Lr= %.5f'\n",
    "                          % (val_ca, val_nmi, val_ari, val_loss[0], val_loss[1], val_loss[2]))\n",
    "                \n",
    "                #maj du dictionnaire \n",
    "                history[\"CA\"].append(ca)\n",
    "                history[\"NMI\"].append(nmi)\n",
    "                history[\"ARI\"].append(ari)\n",
    "                history[\"val_CA\"].append(val_ca)\n",
    "                history[\"val_NMI\"].append(val_nmi)\n",
    "                history[\"val_ARI\"].append(val_ari)\n",
    "                history[\"loss\"].append(loss[0])\n",
    "                history[\"clustering_loss\"].append(loss[1])\n",
    "                history[\"zinb_loss\"].append(loss[2])\n",
    "                history[\"val_loss\"].append(val_loss[0])\n",
    "                history[\"val_clustering_loss\"].append(val_loss[1])\n",
    "                history[\"val_zinb_loss\"].append(val_loss[2])\n",
    "                history[\"kmeans_loss\"].append(loss[3])\n",
    "                history[\"val_kmeans\"].append(val_loss[3])\n",
    "                \n",
    "                if iteration==0:\n",
    "                    history['CA_initial']=ca\n",
    "                    history['ARI_initial']=ari\n",
    "                    history['NMI_initial']=nmi\n",
    "                    history['val_CA_initial']=val_ca\n",
    "                    history['val_ARI_initial']=val_ari\n",
    "                    history['val_NMI_initial']=val_nmi\n",
    "                    \n",
    "            \n",
    "            #save a plot\n",
    "            save_plot_umap(model, x_train, size_factors_train, y_train, y_pred_train, n_cluster, iteration, pdf_pages_train, \"train\")\n",
    "            save_plot_umap(model, x_test, size_factors_test, y_test, y_pred_test, n_cluster, iteration, pdf_pages_test, \"test\")\n",
    "\n",
    "\n",
    "                \n",
    "            #stop criterion\n",
    "            delta_label=np.sum(y_pred_train != y_pred_last_train).astype(np.float32)/y_pred_train.shape[0]\n",
    "            y_pred_last_train=np.copy(y_pred_train)\n",
    "            if iteration >0 and delta_label<tol:\n",
    "                print('delta_label ', delta_label, '< tol ', tol)\n",
    "                print('Reached tolerance threshold. Stopping training.')\n",
    "                break\n",
    "    \n",
    "        if (index + 1)*batch_size > x_train.shape[0]:\n",
    "            model.train_on_batch(x=[x_train[index * batch_size::], size_factors_train[index * batch_size:]],\n",
    "                                                 y=[p_train[index * batch_size::], raw_train[index * batch_size::], cluster_centers])\n",
    "            index=0\n",
    "        else:\n",
    "            model.train_on_batch(x=[x_train[index * batch_size:(index + 1) * batch_size], \n",
    "                                                    size_factors_train[index * batch_size:(index + 1) * batch_size]],\n",
    "                                                 y=[p_train[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    raw_train[index * batch_size:(index + 1) * batch_size], cluster_centers])\n",
    "            index += 1\n",
    "        \n",
    "        #if iteration % save_interval == 0:\n",
    "            # save scDeepCluster model checkpoints\n",
    "            #print('saving model to: ''/weights' + str(iteration) + '.h5')\n",
    "            #model.save_weights('/weights' + str(iteration) + '.h5')\n",
    "            #print('saving model to: model/weights.weights.h5')\n",
    "            #model.save_weights('model/weights.weights.h5')\n",
    "    \n",
    "        #iteration+=1\n",
    "    \n",
    "    ca = np.round(cluster_acc(y_train, y_pred_train), 5)\n",
    "    nmi = np.round(normalized_mutual_info_score(y_train, y_pred_train), 5)\n",
    "    ari = np.round(adjusted_rand_score(y_train, y_pred_train), 5)\n",
    "    val_ca = np.round(cluster_acc(y_test, y_pred_test), 5)\n",
    "    val_nmi = np.round(normalized_mutual_info_score(y_test, y_pred_test), 5)\n",
    "    val_ari = np.round(adjusted_rand_score(y_test, y_pred_test), 5)\n",
    "    print('Final: CA=%.4f, NMI= %.4f, ARI= %.4f' % (ca, nmi, ari))\n",
    "    print('Final: val_CA=%.4f, val_NMI= %.4f, val_ARI= %.4f' % (val_ca, val_nmi, val_ari))\n",
    "    duration=int(time.time() - t0)\n",
    "    print('Clustering time: %d seconds.' % duration )\n",
    "    history[\"training_time\"]=duration\n",
    "    history[\"nbr_iteration\"]=iteration\n",
    "    history['CA_final']=ca\n",
    "    history['ARI_final']=ari\n",
    "    history['NMI_final']=nmi\n",
    "    history['val_CA_final']=val_ca\n",
    "    history['val_ARI_final']=val_ari\n",
    "    history['val_NMI_final']=val_nmi\n",
    "                    \n",
    "    \n",
    "    #save dictionnary\n",
    "    save_results_to_csv(history, 'data/scziDesk/results_scziDesk_AE.csv')\n",
    "    \n",
    "    pdf_pages_train.close()\n",
    "    pdf_pages_test.close()\n",
    "    \n",
    "    return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959aa49-05a2-4d89-8162-ae976350b58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred, model=fit_and_split(data_ann.X, data_ann.obs,  data_ann.obs.size_factors, data_ann.raw.X, method=\"kmeans\", n_cluster=14,  alpha=1.0, noise=0.5, batch_size=256,\n",
    "             max_iter=2e3, tol=1e-3, update_interval=27, loss_weights=[1.0,1.0,1.0],\n",
    "             pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e1c84-494f-47d8-913b-6825cf9489ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_scivar",
   "language": "python",
   "name": "env_scivar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
