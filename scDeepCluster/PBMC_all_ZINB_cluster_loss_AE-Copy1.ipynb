{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953226f9",
   "metadata": {},
   "source": [
    "# First version of AE with clustering loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f30b30",
   "metadata": {},
   "source": [
    "## Import useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2d3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scanpy in /shared/home/tberthet/.local/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: anndata>=0.8 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.10.7)\n",
      "Requirement already satisfied: h5py>=3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (3.11.0)\n",
      "Requirement already satisfied: joblib in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.4.2)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (3.8.4)\n",
      "Requirement already satisfied: natsort in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: networkx>=2.7 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (3.3)\n",
      "Requirement already satisfied: numba>=0.56 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.59.1)\n",
      "Requirement already satisfied: numpy>=1.23 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scanpy) (1.26.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scanpy) (23.2)\n",
      "Requirement already satisfied: pandas>=1.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scanpy) (2.2.0)\n",
      "Requirement already satisfied: patsy in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.5.6)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.5.12)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.8 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.13.0)\n",
      "Requirement already satisfied: seaborn>=0.13 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.13.2)\n",
      "Requirement already satisfied: session-info in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.14.2)\n",
      "Requirement already satisfied: tqdm in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (4.66.4)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scanpy) (0.5.6)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from anndata>=0.8->scanpy) (1.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from numba>=0.56->scanpy) (0.42.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas>=1.5->scanpy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas>=1.5->scanpy) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
      "Requirement already satisfied: six in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from patsy->scanpy) (1.16.0)\n",
      "Requirement already satisfied: stdlib-list in /shared/home/tberthet/.local/lib/python3.12/site-packages (from session-info->scanpy) (0.10.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /shared/home/tberthet/.local/lib/python3.12/site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from optree->keras) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from rich->keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /shared/home/tberthet/.local/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (69.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: rich in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /shared/home/tberthet/.local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /shared/home/tberthet/.local/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: igraph in /shared/home/tberthet/.local/lib/python3.12/site-packages (0.11.5)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from igraph) (1.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: leidenalg in /shared/home/tberthet/.local/lib/python3.12/site-packages (0.10.2)\n",
      "Requirement already satisfied: igraph<0.12,>=0.10.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from leidenalg) (0.11.5)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from igraph<0.12,>=0.10.0->leidenalg) (1.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for csv\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: h5py in /shared/home/tberthet/.local/lib/python3.12/site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from h5py) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /shared/home/tberthet/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /shared/ifbstor1/software/miniconda/envs/jupyterlab-3.5.0/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /shared/home/tberthet/.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scanpy\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip3 install igraph\n",
    "!pip3 install leidenalg\n",
    "!pip install os\n",
    "!pip install csv\n",
    "!pip install h5py\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436ae1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 09:33:31.098565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-03 09:33:42.352207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Layer\n",
    "from keras.models import load_model, Model\n",
    "from keras import backend as K\n",
    "from keras.losses import KLDivergence\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, accuracy_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.layers import Layer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import h5py\n",
    "import run_scDeepCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9baf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c088e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4adc08a-623e-428c-a88e-dc23f52ae650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared/home/tberthet/.local/lib/python3.12/site-packages/anndata/_core/anndata.py:430: FutureWarning: The dtype argument is deprecated and will be removed in late 2024.\n",
      "  warnings.warn(\n",
      "### Autoencoder: Successfully preprocessed 16653 genes and 4271 cells.\n",
      "Namespace(n_clusters=8, knn=20, resolution=0.8, select_genes=0, batch_size=256, data_file='10X_PBMC.h5', maxiter=2000, pretrain_epochs=300, gamma=1.0, sigma=2.5, update_interval=1, tol=0.001, ae_weights=None, save_dir='results/scDeepCluster/', ae_weight_file='AE_weights.pth.tar', final_latent_file='final_latent_file.txt', predict_label_file='pred_labels.txt', device='cpu')\n",
      "(4271, 16653)\n",
      "(4271,)\n",
      "scDeepCluster(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=16653, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (_dec_mean): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): MeanAct()\n",
      "  )\n",
      "  (_dec_disp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): DispAct()\n",
      "  )\n",
      "  (_dec_pi): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (zinb_loss): ZINBLoss()\n",
      ")\n",
      "/shared/ifbstor1/projects/scivar/scDeepCluster/scDeepCluster.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  dataset = TensorDataset(torch.Tensor(X), torch.Tensor(X_raw), torch.Tensor(size_factor))\n",
      "Pretraining stage\n",
      "Pretrain epoch   1, ZINB loss: 0.38768397\n",
      "Pretrain epoch   2, ZINB loss: 0.27529891\n",
      "Pretrain epoch   3, ZINB loss: 0.25922904\n",
      "Pretrain epoch   4, ZINB loss: 0.25235058\n",
      "Pretrain epoch   5, ZINB loss: 0.24861271\n",
      "Pretrain epoch   6, ZINB loss: 0.24572974\n",
      "Pretrain epoch   7, ZINB loss: 0.24350419\n",
      "Pretrain epoch   8, ZINB loss: 0.24220864\n",
      "Pretrain epoch   9, ZINB loss: 0.24090222\n",
      "Pretrain epoch  10, ZINB loss: 0.23960615\n",
      "Pretrain epoch  11, ZINB loss: 0.23890577\n",
      "Pretrain epoch  12, ZINB loss: 0.23819457\n",
      "Pretrain epoch  13, ZINB loss: 0.23755978\n",
      "Pretrain epoch  14, ZINB loss: 0.23692428\n",
      "Pretrain epoch  15, ZINB loss: 0.23652429\n",
      "Pretrain epoch  16, ZINB loss: 0.23599799\n",
      "Pretrain epoch  17, ZINB loss: 0.23573615\n",
      "Pretrain epoch  18, ZINB loss: 0.23546315\n",
      "Pretrain epoch  19, ZINB loss: 0.23516247\n",
      "Pretrain epoch  20, ZINB loss: 0.23491062\n",
      "Pretrain epoch  21, ZINB loss: 0.23471158\n",
      "Pretrain epoch  22, ZINB loss: 0.23452342\n",
      "Pretrain epoch  23, ZINB loss: 0.23452090\n",
      "Pretrain epoch  24, ZINB loss: 0.23424750\n",
      "Pretrain epoch  25, ZINB loss: 0.23411714\n",
      "Pretrain epoch  26, ZINB loss: 0.23402649\n",
      "Pretrain epoch  27, ZINB loss: 0.23382831\n",
      "Pretrain epoch  28, ZINB loss: 0.23372104\n",
      "Pretrain epoch  29, ZINB loss: 0.23359919\n",
      "Pretrain epoch  30, ZINB loss: 0.23348690\n",
      "Pretrain epoch  31, ZINB loss: 0.23346448\n",
      "Pretrain epoch  32, ZINB loss: 0.23333117\n",
      "Pretrain epoch  33, ZINB loss: 0.23325282\n",
      "Pretrain epoch  34, ZINB loss: 0.23312686\n",
      "Pretrain epoch  35, ZINB loss: 0.23306238\n",
      "Pretrain epoch  36, ZINB loss: 0.23299698\n",
      "Pretrain epoch  37, ZINB loss: 0.23283919\n",
      "Pretrain epoch  38, ZINB loss: 0.23274996\n",
      "Pretrain epoch  39, ZINB loss: 0.23269010\n",
      "Pretrain epoch  40, ZINB loss: 0.23267259\n",
      "Pretrain epoch  41, ZINB loss: 0.23260675\n",
      "Pretrain epoch  42, ZINB loss: 0.23249846\n",
      "Pretrain epoch  43, ZINB loss: 0.23239145\n",
      "Pretrain epoch  44, ZINB loss: 0.23228221\n",
      "Pretrain epoch  45, ZINB loss: 0.23223926\n",
      "Pretrain epoch  46, ZINB loss: 0.23213823\n",
      "Pretrain epoch  47, ZINB loss: 0.23205267\n",
      "Pretrain epoch  48, ZINB loss: 0.23199967\n",
      "Pretrain epoch  49, ZINB loss: 0.23194465\n",
      "Pretrain epoch  50, ZINB loss: 0.23189208\n",
      "Pretrain epoch  51, ZINB loss: 0.23181494\n",
      "Pretrain epoch  52, ZINB loss: 0.23171801\n",
      "Pretrain epoch  53, ZINB loss: 0.23164986\n",
      "Pretrain epoch  54, ZINB loss: 0.23159908\n",
      "Pretrain epoch  55, ZINB loss: 0.23150570\n",
      "Pretrain epoch  56, ZINB loss: 0.23150217\n",
      "Pretrain epoch  57, ZINB loss: 0.23138716\n",
      "Pretrain epoch  58, ZINB loss: 0.23131785\n",
      "Pretrain epoch  59, ZINB loss: 0.23137540\n",
      "Pretrain epoch  60, ZINB loss: 0.23117841\n",
      "Pretrain epoch  61, ZINB loss: 0.23120266\n",
      "Pretrain epoch  62, ZINB loss: 0.23112936\n",
      "Pretrain epoch  63, ZINB loss: 0.23108302\n",
      "Pretrain epoch  64, ZINB loss: 0.23102559\n",
      "Pretrain epoch  98, ZINB loss: 0.22888228\n",
      "Pretrain epoch  99, ZINB loss: 0.22907200\n",
      "Pretrain epoch 100, ZINB loss: 0.22882177\n",
      "Pretrain epoch 101, ZINB loss: 0.22862771\n",
      "Pretrain epoch 102, ZINB loss: 0.22850498\n",
      "Pretrain epoch 103, ZINB loss: 0.22844374\n",
      "Pretrain epoch 104, ZINB loss: 0.22840157\n",
      "Pretrain epoch 105, ZINB loss: 0.22841186\n",
      "Pretrain epoch 106, ZINB loss: 0.22848726\n",
      "Pretrain epoch 107, ZINB loss: 0.22834296\n",
      "Pretrain epoch 108, ZINB loss: 0.22819362\n",
      "Pretrain epoch 109, ZINB loss: 0.22808027\n",
      "Pretrain epoch 110, ZINB loss: 0.22804222\n",
      "Pretrain epoch 111, ZINB loss: 0.22799366\n",
      "Pretrain epoch 112, ZINB loss: 0.22792937\n",
      "Pretrain epoch 113, ZINB loss: 0.22786671\n",
      "Pretrain epoch 114, ZINB loss: 0.22778548\n",
      "Pretrain epoch 115, ZINB loss: 0.22774896\n",
      "Pretrain epoch 116, ZINB loss: 0.22768004\n",
      "Pretrain epoch 117, ZINB loss: 0.22768485\n",
      "Pretrain epoch 118, ZINB loss: 0.22764763\n",
      "Pretrain epoch 119, ZINB loss: 0.22748831\n",
      "Pretrain epoch 120, ZINB loss: 0.22753227\n",
      "Pretrain epoch 121, ZINB loss: 0.22755406\n",
      "Pretrain epoch 122, ZINB loss: 0.22736371\n",
      "Pretrain epoch 123, ZINB loss: 0.22725107\n",
      "Pretrain epoch 124, ZINB loss: 0.22713283\n",
      "Pretrain epoch 125, ZINB loss: 0.22711893\n",
      "Pretrain epoch 126, ZINB loss: 0.22707457\n",
      "Pretrain epoch 127, ZINB loss: 0.22700354\n",
      "Pretrain epoch 128, ZINB loss: 0.22701507\n",
      "Pretrain epoch 129, ZINB loss: 0.22708818\n",
      "Pretrain epoch 130, ZINB loss: 0.22679189\n",
      "Pretrain epoch 131, ZINB loss: 0.22675714\n",
      "Pretrain epoch 133, ZINB loss: 0.22666008\n",
      "Pretrain epoch 134, ZINB loss: 0.22657635\n",
      "Pretrain epoch 135, ZINB loss: 0.22652503\n",
      "Pretrain epoch 136, ZINB loss: 0.22642025\n",
      "Pretrain epoch 137, ZINB loss: 0.22643193\n",
      "Pretrain epoch 138, ZINB loss: 0.22632746\n",
      "Pretrain epoch 139, ZINB loss: 0.22632152\n",
      "Pretrain epoch 140, ZINB loss: 0.22626177\n",
      "Pretrain epoch 141, ZINB loss: 0.22617639\n",
      "Pretrain epoch 142, ZINB loss: 0.22632468\n",
      "Pretrain epoch 143, ZINB loss: 0.22605482\n",
      "Pretrain epoch 144, ZINB loss: 0.22591459\n",
      "Pretrain epoch 145, ZINB loss: 0.22586346\n",
      "Pretrain epoch 146, ZINB loss: 0.22568403\n",
      "Pretrain epoch 147, ZINB loss: 0.22566559\n",
      "Pretrain epoch 148, ZINB loss: 0.22572592\n",
      "Pretrain epoch 149, ZINB loss: 0.22583333\n",
      "Pretrain epoch 150, ZINB loss: 0.22578781\n",
      "Pretrain epoch 151, ZINB loss: 0.22552772\n",
      "Pretrain epoch 152, ZINB loss: 0.22540116\n",
      "Pretrain epoch 153, ZINB loss: 0.22533436\n",
      "Pretrain epoch 154, ZINB loss: 0.22527500\n",
      "Pretrain epoch 155, ZINB loss: 0.22513294\n",
      "Pretrain epoch 156, ZINB loss: 0.22512896\n",
      "Pretrain epoch 157, ZINB loss: 0.22518315\n",
      "Pretrain epoch 158, ZINB loss: 0.22504479\n",
      "Pretrain epoch 159, ZINB loss: 0.22490729\n",
      "Pretrain epoch 160, ZINB loss: 0.22503393\n",
      "Pretrain epoch 161, ZINB loss: 0.22488711\n",
      "Pretrain epoch 162, ZINB loss: 0.22472115\n",
      "Pretrain epoch 163, ZINB loss: 0.22467824\n",
      "Pretrain epoch 164, ZINB loss: 0.22452158\n",
      "Pretrain epoch 165, ZINB loss: 0.22448134\n",
      "Pretrain epoch 166, ZINB loss: 0.22445736\n",
      "Pretrain epoch 167, ZINB loss: 0.22428354\n",
      "Pretrain epoch 168, ZINB loss: 0.22437403\n",
      "Pretrain epoch 169, ZINB loss: 0.22423491\n",
      "Pretrain epoch 170, ZINB loss: 0.22413696\n",
      "Pretrain epoch 171, ZINB loss: 0.22419390\n",
      "Pretrain epoch 200, ZINB loss: 0.22249878\n",
      "Pretrain epoch 201, ZINB loss: 0.22232188\n",
      "Pretrain epoch 202, ZINB loss: 0.22212743\n",
      "Pretrain epoch 203, ZINB loss: 0.22188131\n",
      "Pretrain epoch 204, ZINB loss: 0.22170900\n",
      "Pretrain epoch 205, ZINB loss: 0.22177073\n",
      "Pretrain epoch 206, ZINB loss: 0.22189990\n",
      "Pretrain epoch 207, ZINB loss: 0.22170154\n",
      "Pretrain epoch 208, ZINB loss: 0.22162240\n",
      "Pretrain epoch 209, ZINB loss: 0.22147162\n",
      "Pretrain epoch 210, ZINB loss: 0.22135532\n",
      "Pretrain epoch 211, ZINB loss: 0.22131885\n",
      "Pretrain epoch 212, ZINB loss: 0.22125593\n",
      "Pretrain epoch 213, ZINB loss: 0.22122882\n",
      "Pretrain epoch 214, ZINB loss: 0.22110973\n",
      "Pretrain epoch 215, ZINB loss: 0.22107735\n",
      "Pretrain epoch 216, ZINB loss: 0.22100132\n",
      "Pretrain epoch 217, ZINB loss: 0.22128679\n",
      "Pretrain epoch 218, ZINB loss: 0.22099734\n",
      "Pretrain epoch 219, ZINB loss: 0.22085108\n",
      "Pretrain epoch 220, ZINB loss: 0.22078883\n",
      "Pretrain epoch 221, ZINB loss: 0.22069842\n",
      "Pretrain epoch 222, ZINB loss: 0.22051669\n",
      "Pretrain epoch 223, ZINB loss: 0.22054398\n",
      "Pretrain epoch 224, ZINB loss: 0.22120324\n",
      "Pretrain epoch 225, ZINB loss: 0.22114988\n",
      "Pretrain epoch 226, ZINB loss: 0.22071854\n",
      "Pretrain epoch 227, ZINB loss: 0.22034802\n",
      "Pretrain epoch 228, ZINB loss: 0.22023234\n",
      "Pretrain epoch 229, ZINB loss: 0.22017535\n",
      "Pretrain epoch 230, ZINB loss: 0.22008795\n",
      "Pretrain epoch 231, ZINB loss: 0.21997159\n",
      "Pretrain epoch 232, ZINB loss: 0.21991598\n",
      "Pretrain epoch 233, ZINB loss: 0.21987086\n",
      "Pretrain epoch 234, ZINB loss: 0.21991760\n",
      "Pretrain epoch 235, ZINB loss: 0.21977015\n",
      "Pretrain epoch 236, ZINB loss: 0.21969828\n",
      "Pretrain epoch 237, ZINB loss: 0.21969630\n",
      "Pretrain epoch 238, ZINB loss: 0.21958247\n",
      "Pretrain epoch 239, ZINB loss: 0.21948781\n",
      "Pretrain epoch 240, ZINB loss: 0.21941205\n",
      "Pretrain epoch 241, ZINB loss: 0.21930444\n",
      "Pretrain epoch 242, ZINB loss: 0.21929111\n",
      "Pretrain epoch 243, ZINB loss: 0.21934294\n",
      "Pretrain epoch 244, ZINB loss: 0.21930558\n",
      "Pretrain epoch 245, ZINB loss: 0.21932257\n",
      "Pretrain epoch 246, ZINB loss: 0.21925394\n",
      "Pretrain epoch 247, ZINB loss: 0.21920488\n",
      "Pretrain epoch 248, ZINB loss: 0.21884114\n",
      "Pretrain epoch 249, ZINB loss: 0.21888341\n",
      "Pretrain epoch 250, ZINB loss: 0.21886405\n",
      "Pretrain epoch 251, ZINB loss: 0.21871166\n",
      "Pretrain epoch 252, ZINB loss: 0.21854335\n",
      "Pretrain epoch 253, ZINB loss: 0.21873423\n",
      "Pretrain epoch 254, ZINB loss: 0.21866367\n",
      "Pretrain epoch 255, ZINB loss: 0.21846443\n",
      "Pretrain epoch 256, ZINB loss: 0.21843371\n",
      "Pretrain epoch 257, ZINB loss: 0.21837158\n",
      "Pretrain epoch 258, ZINB loss: 0.21830941\n",
      "Pretrain epoch 259, ZINB loss: 0.21835937\n",
      "Pretrain epoch 260, ZINB loss: 0.21842403\n",
      "Pretrain epoch 261, ZINB loss: 0.21809538\n",
      "Pretrain epoch 262, ZINB loss: 0.21796602\n",
      "Pretrain epoch 263, ZINB loss: 0.21805932\n",
      "Pretrain epoch 264, ZINB loss: 0.21821026\n",
      "Pretrain epoch 265, ZINB loss: 0.21799891\n",
      "Pretrain epoch 266, ZINB loss: 0.21785735\n",
      "Pretrain epoch 267, ZINB loss: 0.21794994\n",
      "Pretrain epoch 268, ZINB loss: 0.21790489\n",
      "Pretrain epoch 269, ZINB loss: 0.21773189\n",
      "Pretrain epoch 270, ZINB loss: 0.21761517\n",
      "Pretrain epoch 271, ZINB loss: 0.21764239\n",
      "Pretrain epoch 272, ZINB loss: 0.21756048\n",
      "Pretrain epoch 273, ZINB loss: 0.21741534\n",
      "Pretrain epoch 274, ZINB loss: 0.21725112\n",
      "Pretrain epoch 275, ZINB loss: 0.21743276\n",
      "Pretrain epoch 276, ZINB loss: 0.21735271\n",
      "Pretrain epoch 277, ZINB loss: 0.21710305\n",
      "Pretrain epoch 278, ZINB loss: 0.21710589\n",
      "Pretrain epoch 279, ZINB loss: 0.21727840\n",
      "Pretrain epoch 280, ZINB loss: 0.21720685\n",
      "Pretrain epoch 281, ZINB loss: 0.21691456\n",
      "Pretrain epoch 282, ZINB loss: 0.21696434\n",
      "Pretrain epoch 283, ZINB loss: 0.21665856\n",
      "Pretrain epoch 284, ZINB loss: 0.21687970\n",
      "Pretrain epoch 285, ZINB loss: 0.21664697\n",
      "Pretrain epoch 286, ZINB loss: 0.21656651\n",
      "Pretrain epoch 287, ZINB loss: 0.21663336\n",
      "Pretrain epoch 288, ZINB loss: 0.21669737\n",
      "Pretrain epoch 289, ZINB loss: 0.21643810\n",
      "Pretrain epoch 290, ZINB loss: 0.21644094\n",
      "Pretrain epoch 291, ZINB loss: 0.21641986\n",
      "Pretrain epoch 292, ZINB loss: 0.21626543\n",
      "Pretrain epoch 294, ZINB loss: 0.21652236\n",
      "Pretrain epoch 295, ZINB loss: 0.21620052\n",
      "Pretrain epoch 296, ZINB loss: 0.21621379\n",
      "Pretrain epoch 297, ZINB loss: 0.21635335\n",
      "Pretrain epoch 298, ZINB loss: 0.21658632\n",
      "Pretrain epoch 299, ZINB loss: 0.21605899\n",
      "Pretrain epoch 300, ZINB loss: 0.21611185\n",
      "Pretraining time: 17569 seconds.\n",
      "Clustering stage\n",
      "/shared/ifbstor1/projects/scivar/scDeepCluster/scDeepCluster.py:149: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factor = torch.tensor(size_factor, dtype=torch.float64)\n",
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.7222, ARI= 0.6789, ACC=0.7282\n",
      "Clustering   1: NMI= 0.7222, ARI= 0.6789, ACC=0.7282\n",
      "Epoch   1: Total: 0.40859229 Clustering Loss: 0.13298564 ZINB Loss: 0.27560665\n",
      "Clustering   2: NMI= 0.7423, ARI= 0.6871, ACC=0.7371\n",
      "Epoch   2: Total: 0.46919940 Clustering Loss: 0.21948968 ZINB Loss: 0.24970972\n",
      "Clustering   3: NMI= 0.7363, ARI= 0.6820, ACC=0.7415\n",
      "Epoch   3: Total: 0.52286103 Clustering Loss: 0.27079772 ZINB Loss: 0.25206331\n",
      "Clustering   4: NMI= 0.7231, ARI= 0.6944, ACC=0.7195\n",
      "Epoch   4: Total: 0.48887387 Clustering Loss: 0.24144668 ZINB Loss: 0.24742719\n",
      "Clustering   5: NMI= 0.7389, ARI= 0.7076, ACC=0.7504\n",
      "Epoch   5: Total: 0.48809862 Clustering Loss: 0.22977720 ZINB Loss: 0.25832143\n",
      "Clustering   6: NMI= 0.7190, ARI= 0.6665, ACC=0.7062\n",
      "Epoch   6: Total: 0.48863864 Clustering Loss: 0.23520648 ZINB Loss: 0.25343216\n",
      "Clustering   7: NMI= 0.7518, ARI= 0.7038, ACC=0.7401\n",
      "Epoch   7: Total: 0.46960607 Clustering Loss: 0.21596799 ZINB Loss: 0.25363808\n",
      "Clustering   8: NMI= 0.7260, ARI= 0.6656, ACC=0.7066\n",
      "Epoch   8: Total: 0.46852856 Clustering Loss: 0.21446274 ZINB Loss: 0.25406582\n",
      "Clustering   9: NMI= 0.7504, ARI= 0.6954, ACC=0.7321\n",
      "Epoch   9: Total: 0.47158236 Clustering Loss: 0.21684174 ZINB Loss: 0.25474062\n",
      "Clustering   10: NMI= 0.7264, ARI= 0.6602, ACC=0.7026\n",
      "Epoch  10: Total: 0.49380352 Clustering Loss: 0.24277982 ZINB Loss: 0.25102370\n",
      "Clustering   11: NMI= 0.7480, ARI= 0.6902, ACC=0.7232\n",
      "Epoch  11: Total: 0.43724478 Clustering Loss: 0.18858666 ZINB Loss: 0.24865812\n",
      "Clustering   12: NMI= 0.7321, ARI= 0.6662, ACC=0.7073\n",
      "Epoch  12: Total: 0.47362241 Clustering Loss: 0.22591330 ZINB Loss: 0.24770911\n",
      "Clustering   13: NMI= 0.7457, ARI= 0.6857, ACC=0.7221\n",
      "Epoch  13: Total: 0.43834023 Clustering Loss: 0.18579587 ZINB Loss: 0.25254436\n",
      "Clustering   14: NMI= 0.7353, ARI= 0.6683, ACC=0.7104\n",
      "Epoch  14: Total: 0.42658694 Clustering Loss: 0.17379148 ZINB Loss: 0.25279546\n",
      "Clustering   15: NMI= 0.7475, ARI= 0.6847, ACC=0.7226\n",
      "Epoch  15: Total: 0.42814968 Clustering Loss: 0.17865536 ZINB Loss: 0.24949432\n",
      "Clustering   16: NMI= 0.7516, ARI= 0.6867, ACC=0.7380\n",
      "Epoch  16: Total: 0.48851178 Clustering Loss: 0.23811383 ZINB Loss: 0.25039796\n",
      "Clustering   17: NMI= 0.7584, ARI= 0.6992, ACC=0.7497\n",
      "Epoch  17: Total: 0.42208577 Clustering Loss: 0.17008332 ZINB Loss: 0.25200245\n",
      "Clustering   18: NMI= 0.7508, ARI= 0.6849, ACC=0.7279\n",
      "Epoch  18: Total: 0.44582821 Clustering Loss: 0.19751117 ZINB Loss: 0.24831704\n",
      "Clustering   19: NMI= 0.7580, ARI= 0.6949, ACC=0.7401\n",
      "Epoch  19: Total: 0.40966530 Clustering Loss: 0.16140867 ZINB Loss: 0.24825663\n",
      "Clustering   20: NMI= 0.7477, ARI= 0.6810, ACC=0.7228\n",
      "Epoch  20: Total: 0.42766803 Clustering Loss: 0.18384656 ZINB Loss: 0.24382147\n",
      "Clustering   21: NMI= 0.7531, ARI= 0.6903, ACC=0.7336\n",
      "Epoch  21: Total: 0.40417904 Clustering Loss: 0.15226907 ZINB Loss: 0.25190998\n",
      "Clustering   22: NMI= 0.7454, ARI= 0.6791, ACC=0.7218\n",
      "Epoch  22: Total: 0.39921772 Clustering Loss: 0.15283462 ZINB Loss: 0.24638310\n",
      "Clustering   23: NMI= 0.7512, ARI= 0.6867, ACC=0.7275\n",
      "Epoch  23: Total: 0.40133704 Clustering Loss: 0.15837911 ZINB Loss: 0.24295794\n",
      "Clustering   24: NMI= 0.7460, ARI= 0.6807, ACC=0.7226\n",
      "Epoch  24: Total: 0.39243367 Clustering Loss: 0.14079511 ZINB Loss: 0.25163856\n",
      "Clustering   25: NMI= 0.7530, ARI= 0.6900, ACC=0.7329\n",
      "Epoch  25: Total: 0.38466687 Clustering Loss: 0.13810662 ZINB Loss: 0.24656024\n",
      "Clustering   26: NMI= 0.7502, ARI= 0.6863, ACC=0.7300\n",
      "Epoch  26: Total: 0.42226805 Clustering Loss: 0.18024358 ZINB Loss: 0.24202447\n",
      "Clustering   27: NMI= 0.7540, ARI= 0.6915, ACC=0.7380\n",
      "Epoch  27: Total: 0.37418798 Clustering Loss: 0.12501265 ZINB Loss: 0.24917533\n",
      "Clustering   28: NMI= 0.7510, ARI= 0.6880, ACC=0.7347\n",
      "Epoch  28: Total: 0.40084685 Clustering Loss: 0.15634478 ZINB Loss: 0.24450207\n",
      "Clustering   29: NMI= 0.7525, ARI= 0.6908, ACC=0.7375\n",
      "Epoch  29: Total: 0.38449353 Clustering Loss: 0.13641931 ZINB Loss: 0.24807422\n",
      "Clustering   30: NMI= 0.7497, ARI= 0.6866, ACC=0.7321\n",
      "Epoch  30: Total: 0.37040407 Clustering Loss: 0.12648430 ZINB Loss: 0.24391977\n",
      "Clustering   31: NMI= 0.7487, ARI= 0.6891, ACC=0.7357\n",
      "Epoch  31: Total: 0.37199000 Clustering Loss: 0.12767683 ZINB Loss: 0.24431317\n",
      "Clustering   32: NMI= 0.7486, ARI= 0.6858, ACC=0.7298\n",
      "Epoch  32: Total: 0.34127015 Clustering Loss: 0.09608584 ZINB Loss: 0.24518432\n",
      "Clustering   33: NMI= 0.7512, ARI= 0.6885, ACC=0.7347\n",
      "Epoch  33: Total: 0.36878799 Clustering Loss: 0.12126731 ZINB Loss: 0.24752068\n",
      "Clustering   34: NMI= 0.7495, ARI= 0.6863, ACC=0.7312\n",
      "Epoch  34: Total: 0.35473874 Clustering Loss: 0.11164707 ZINB Loss: 0.24309167\n",
      "Clustering   35: NMI= 0.7482, ARI= 0.6870, ACC=0.7340\n",
      "Epoch  35: Total: 0.34656491 Clustering Loss: 0.10138662 ZINB Loss: 0.24517829\n",
      "Clustering   36: NMI= 0.7486, ARI= 0.6872, ACC=0.7338\n",
      "delta_label  0.0007024116132053383 < tol  0.001\n",
      "Reach tolerance threshold. Stopping training.\n",
      "Total time: 20134 seconds.\n",
      "Evaluating cells: NMI= 0.7486, ARI= 0.6872, ACC = 0.7338\n"
     ]
    }
   ],
   "source": [
    "!python run_scDeepCluster.py --data_file 10X_PBMC.h5 --n_clusters 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ac00e-aeb2-44af-b45c-9e26c304cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared/home/tberthet/.local/lib/python3.12/site-packages/anndata/_core/anndata.py:430: FutureWarning: The dtype argument is deprecated and will be removed in late 2024.\n",
      "  warnings.warn(\n",
      "### Autoencoder: Successfully preprocessed 16653 genes and 4271 cells.\n",
      "Namespace(n_clusters=8, knn=20, resolution=0.8, select_genes=0, batch_size=256, data_file='10X_PBMC.h5', maxiter=2000, pretrain_epochs=300, gamma=1.0, sigma=2.5, update_interval=1, tol=0.001, ae_weights=None, save_dir='results/scDeepCluster/', ae_weight_file='AE_weights.pth.tar', final_latent_file='final_latent_file.txt', predict_label_file='pred_labels.txt', device='cpu')\n",
      "(4271, 16653)\n",
      "(4271,)\n",
      "scDeepCluster(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=16653, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (_dec_mean): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): MeanAct()\n",
      "  )\n",
      "  (_dec_disp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): DispAct()\n",
      "  )\n",
      "  (_dec_pi): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (zinb_loss): ZINBLoss()\n",
      ")\n",
      "/shared/ifbstor1/projects/scivar/scDeepCluster/scDeepCluster.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  dataset = TensorDataset(torch.Tensor(X), torch.Tensor(X_raw), torch.Tensor(size_factor))\n",
      "Pretraining stage\n",
      "Pretrain epoch   1, ZINB loss: 0.38768397\n",
      "Pretrain epoch   2, ZINB loss: 0.27529891\n",
      "Pretrain epoch   3, ZINB loss: 0.25922904\n",
      "Pretrain epoch   4, ZINB loss: 0.25235058\n",
      "Pretrain epoch   5, ZINB loss: 0.24861271\n",
      "Pretrain epoch   6, ZINB loss: 0.24572974\n",
      "Pretrain epoch   7, ZINB loss: 0.24350419\n",
      "Pretrain epoch   8, ZINB loss: 0.24220864\n",
      "Pretrain epoch   9, ZINB loss: 0.24090222\n",
      "Pretrain epoch  10, ZINB loss: 0.23960615\n",
      "Pretrain epoch  11, ZINB loss: 0.23890577\n",
      "Pretrain epoch  12, ZINB loss: 0.23819457\n",
      "Pretrain epoch  13, ZINB loss: 0.23755978\n",
      "Pretrain epoch  14, ZINB loss: 0.23692428\n",
      "Pretrain epoch  15, ZINB loss: 0.23652429\n",
      "Pretrain epoch  16, ZINB loss: 0.23599799\n",
      "Pretrain epoch  17, ZINB loss: 0.23573615\n",
      "Pretrain epoch  18, ZINB loss: 0.23546315\n",
      "Pretrain epoch  19, ZINB loss: 0.23516247\n",
      "Pretrain epoch  20, ZINB loss: 0.23491062\n",
      "Pretrain epoch  21, ZINB loss: 0.23471158\n",
      "Pretrain epoch  22, ZINB loss: 0.23452342\n",
      "Pretrain epoch  23, ZINB loss: 0.23452090\n",
      "Pretrain epoch  24, ZINB loss: 0.23424750\n",
      "Pretrain epoch  25, ZINB loss: 0.23411714\n",
      "Pretrain epoch  26, ZINB loss: 0.23402649\n",
      "Pretrain epoch  27, ZINB loss: 0.23382831\n",
      "Pretrain epoch  28, ZINB loss: 0.23372104\n",
      "Pretrain epoch  29, ZINB loss: 0.23359919\n",
      "Pretrain epoch  30, ZINB loss: 0.23348690\n",
      "Pretrain epoch  31, ZINB loss: 0.23346448\n",
      "Pretrain epoch  32, ZINB loss: 0.23333117\n",
      "Pretrain epoch  33, ZINB loss: 0.23325282\n",
      "Pretrain epoch  34, ZINB loss: 0.23312686\n",
      "Pretrain epoch  35, ZINB loss: 0.23306238\n",
      "Pretrain epoch  36, ZINB loss: 0.23299698\n",
      "Pretrain epoch  37, ZINB loss: 0.23283919\n",
      "Pretrain epoch  38, ZINB loss: 0.23274996\n",
      "Pretrain epoch  39, ZINB loss: 0.23269010\n",
      "Pretrain epoch  40, ZINB loss: 0.23267259\n",
      "Pretrain epoch  41, ZINB loss: 0.23260675\n",
      "Pretrain epoch  42, ZINB loss: 0.23249846\n",
      "Pretrain epoch  43, ZINB loss: 0.23239145\n",
      "Pretrain epoch  44, ZINB loss: 0.23228221\n",
      "Pretrain epoch  45, ZINB loss: 0.23223926\n",
      "Pretrain epoch  46, ZINB loss: 0.23213823\n",
      "Pretrain epoch  47, ZINB loss: 0.23205267\n",
      "Pretrain epoch  48, ZINB loss: 0.23199967\n",
      "Pretrain epoch  49, ZINB loss: 0.23194465\n",
      "Pretrain epoch  50, ZINB loss: 0.23189208\n",
      "Pretrain epoch  51, ZINB loss: 0.23181494\n",
      "Pretrain epoch  52, ZINB loss: 0.23171801\n",
      "Pretrain epoch  53, ZINB loss: 0.23164986\n",
      "Pretrain epoch  54, ZINB loss: 0.23159908\n",
      "Pretrain epoch  55, ZINB loss: 0.23150570\n",
      "Pretrain epoch  56, ZINB loss: 0.23150217\n",
      "Pretrain epoch  57, ZINB loss: 0.23138716\n",
      "Pretrain epoch  58, ZINB loss: 0.23131785\n",
      "Pretrain epoch  59, ZINB loss: 0.23137540\n",
      "Pretrain epoch  60, ZINB loss: 0.23117841\n",
      "Pretrain epoch  61, ZINB loss: 0.23120266\n",
      "Pretrain epoch  62, ZINB loss: 0.23112936\n",
      "Pretrain epoch  63, ZINB loss: 0.23108302\n",
      "Pretrain epoch  64, ZINB loss: 0.23102559\n",
      "Pretrain epoch  65, ZINB loss: 0.23091564\n",
      "Pretrain epoch  66, ZINB loss: 0.23079203\n",
      "Pretrain epoch  67, ZINB loss: 0.23078243\n",
      "Pretrain epoch  68, ZINB loss: 0.23069352\n",
      "Pretrain epoch  69, ZINB loss: 0.23069890\n",
      "Pretrain epoch  70, ZINB loss: 0.23060448\n",
      "Pretrain epoch  71, ZINB loss: 0.23060886\n",
      "Pretrain epoch  72, ZINB loss: 0.23052291\n",
      "Pretrain epoch  73, ZINB loss: 0.23038769\n",
      "Pretrain epoch  74, ZINB loss: 0.23042768\n",
      "Pretrain epoch  75, ZINB loss: 0.23031169\n",
      "Pretrain epoch  76, ZINB loss: 0.23018802\n",
      "Pretrain epoch  77, ZINB loss: 0.23013602\n",
      "Pretrain epoch  78, ZINB loss: 0.23005996\n",
      "Pretrain epoch  79, ZINB loss: 0.23000668\n",
      "Pretrain epoch  80, ZINB loss: 0.22997036\n",
      "Pretrain epoch  81, ZINB loss: 0.22991248\n",
      "Pretrain epoch  82, ZINB loss: 0.22976013\n",
      "Pretrain epoch  83, ZINB loss: 0.22975434\n",
      "Pretrain epoch  84, ZINB loss: 0.22965028\n",
      "Pretrain epoch  85, ZINB loss: 0.22964401\n",
      "Pretrain epoch  86, ZINB loss: 0.22961363\n",
      "Pretrain epoch  87, ZINB loss: 0.22954346\n",
      "Pretrain epoch  88, ZINB loss: 0.22939773\n",
      "Pretrain epoch  89, ZINB loss: 0.22935373\n",
      "Pretrain epoch  90, ZINB loss: 0.22928592\n",
      "Pretrain epoch  91, ZINB loss: 0.22920626\n",
      "Pretrain epoch  92, ZINB loss: 0.22915233\n",
      "Pretrain epoch  93, ZINB loss: 0.22913799\n",
      "Pretrain epoch  94, ZINB loss: 0.22907678\n",
      "Pretrain epoch  95, ZINB loss: 0.22903795\n",
      "Pretrain epoch  96, ZINB loss: 0.22888517\n",
      "Pretrain epoch  97, ZINB loss: 0.22881247\n",
      "Pretrain epoch  98, ZINB loss: 0.22888228\n",
      "Pretrain epoch  99, ZINB loss: 0.22907200\n",
      "Pretrain epoch 100, ZINB loss: 0.22882177\n",
      "Pretrain epoch 101, ZINB loss: 0.22862771\n",
      "Pretrain epoch 102, ZINB loss: 0.22850498\n",
      "Pretrain epoch 103, ZINB loss: 0.22844374\n",
      "Pretrain epoch 104, ZINB loss: 0.22840157\n",
      "Pretrain epoch 105, ZINB loss: 0.22841186\n",
      "Pretrain epoch 106, ZINB loss: 0.22848726\n",
      "Pretrain epoch 107, ZINB loss: 0.22834296\n",
      "Pretrain epoch 108, ZINB loss: 0.22819362\n",
      "Pretrain epoch 109, ZINB loss: 0.22808027\n",
      "Pretrain epoch 110, ZINB loss: 0.22804222\n",
      "Pretrain epoch 111, ZINB loss: 0.22799366\n",
      "Pretrain epoch 112, ZINB loss: 0.22792937\n",
      "Pretrain epoch 113, ZINB loss: 0.22786671\n",
      "Pretrain epoch 114, ZINB loss: 0.22778548\n",
      "Pretrain epoch 115, ZINB loss: 0.22774896\n",
      "Pretrain epoch 116, ZINB loss: 0.22768004\n",
      "Pretrain epoch 117, ZINB loss: 0.22768485\n",
      "Pretrain epoch 118, ZINB loss: 0.22764763\n",
      "Pretrain epoch 119, ZINB loss: 0.22748831\n",
      "Pretrain epoch 120, ZINB loss: 0.22753227\n",
      "Pretrain epoch 121, ZINB loss: 0.22755406\n",
      "Pretrain epoch 122, ZINB loss: 0.22736371\n",
      "Pretrain epoch 123, ZINB loss: 0.22725107\n",
      "Pretrain epoch 124, ZINB loss: 0.22713283\n",
      "Pretrain epoch 125, ZINB loss: 0.22711893\n",
      "Pretrain epoch 126, ZINB loss: 0.22707457\n",
      "Pretrain epoch 127, ZINB loss: 0.22700354\n",
      "Pretrain epoch 128, ZINB loss: 0.22701507\n"
     ]
    }
   ],
   "source": [
    "!python run_scDeepCluster.py --data_file 10X_PBMC.h5 --n_clusters 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720586b2-a608-4966-ae33-c6a39781867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_scDeepCluster.py --data_file 10X_PBMC.h5 --n_clusters 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8432bf",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaf695",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287976f",
   "metadata": {},
   "source": [
    "On importe le dataset baron et on applique le prtraitement habituel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b855d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16643</th>\n",
       "      <th>16644</th>\n",
       "      <th>16645</th>\n",
       "      <th>16646</th>\n",
       "      <th>16647</th>\n",
       "      <th>16648</th>\n",
       "      <th>16649</th>\n",
       "      <th>16650</th>\n",
       "      <th>16651</th>\n",
       "      <th>16652</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4271 rows  16653 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9      \\\n",
       "0       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3       0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0   \n",
       "4       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "4266    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4267    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4268    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    2.0    0.0   \n",
       "4269    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4270    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      ...  16643  16644  16645  16646  16647  16648  16649  16650  16651  \\\n",
       "0     ...    0.0    3.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1     ...    0.0   14.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2     ...    0.0    7.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3     ...    0.0    8.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4     ...    0.0    4.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "4266  ...    0.0   10.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "4267  ...    0.0    9.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4268  ...    1.0   20.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0   \n",
       "4269  ...    0.0    3.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4270  ...    0.0    9.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      16652  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "...     ...  \n",
       "4266    0.0  \n",
       "4267    0.0  \n",
       "4268    0.0  \n",
       "4269    0.0  \n",
       "4270    0.0  \n",
       "\n",
       "[4271 rows x 16653 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"./dataset/10X_PBMC.h5\"\n",
    "with h5py.File(filename, 'r') as f :\n",
    "    data_X=f['X'][:]\n",
    "    data_Y=f['Y'][:]\n",
    "    df_X=pd.DataFrame(data_X)\n",
    "    df_Y=pd.DataFrame(data_Y)\n",
    "    \n",
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e162be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4271 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label\n",
       "0        2\n",
       "1        2\n",
       "2        2\n",
       "3        8\n",
       "4        3\n",
       "...    ...\n",
       "4266     6\n",
       "4267     5\n",
       "4268     7\n",
       "4269     3\n",
       "4270     2\n",
       "\n",
       "[4271 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Y.columns=['label']\n",
    "df_Y[\"label\"]=df_Y['label'].astype(str)\n",
    "df_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e501a3-52ac-431a-bb23-ffa38014e3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    1292\n",
       "2     702\n",
       "3     606\n",
       "4     459\n",
       "5     450\n",
       "6     332\n",
       "7     295\n",
       "8     135\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Y[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2334f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/tberthet/.local/lib/python3.12/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/shared/home/tberthet/.local/lib/python3.12/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs  n_vars = 4271  16653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann=sc.AnnData(df_X)\n",
    "data_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8599d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.obs=df_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98b9b4",
   "metadata": {},
   "source": [
    "### Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8b136",
   "metadata": {},
   "source": [
    "On filtre les donnes de manire peu stricte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a97bdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/tberthet/.local/lib/python3.12/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "sc.pp.filter_cells(data_ann, min_genes=1)\n",
    "sc.pp.filter_genes(data_ann, min_cells=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d666545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ann.raw = data_ann.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dbcce11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs  n_vars = 4271  16653\n",
       "    obs: 'label', 'n_genes'\n",
       "    var: 'n_cells'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d30f5",
   "metadata": {},
   "source": [
    "### Normalize and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "219882fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(data_ann)\n",
    "data_ann.obs['size_factors'] = data_ann.obs.n_genes / np.median(data_ann.obs.n_genes)\n",
    "#data_ann.obs['size_factors'] = 1.0\n",
    "sc.pp.log1p(data_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "038d051f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1280x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.pp.highly_variable_genes(data_ann, n_top_genes=2000, batch_key=\"label\")\n",
    "sc.pl.highly_variable_genes(data_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "490890e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.scale(data_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb6129",
   "metadata": {},
   "source": [
    "### Explore the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b31b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       ...,\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., 16.46698363,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "930af5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16653"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann.n_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744468b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       ...,\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., 16.46698363,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358],\n",
       "       [-0.04478425, -0.36973775, -0.22519336, ..., -0.04429302,\n",
       "        -0.05613264, -0.09001358]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2814ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_cells</th>\n",
       "      <th>highly_variable</th>\n",
       "      <th>means</th>\n",
       "      <th>dispersions</th>\n",
       "      <th>dispersions_norm</th>\n",
       "      <th>highly_variable_nbatches</th>\n",
       "      <th>highly_variable_intersection</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>-0.013101</td>\n",
       "      <td>-0.060134</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.028724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570</td>\n",
       "      <td>False</td>\n",
       "      <td>0.135255</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>0.209814</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.091207</td>\n",
       "      <td>0.246681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225</td>\n",
       "      <td>False</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>-0.021517</td>\n",
       "      <td>0.112911</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.034340</td>\n",
       "      <td>0.152490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>-0.149268</td>\n",
       "      <td>-0.466780</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.033887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>False</td>\n",
       "      <td>0.024858</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>0.267766</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16648</th>\n",
       "      <td>322</td>\n",
       "      <td>False</td>\n",
       "      <td>0.070651</td>\n",
       "      <td>0.029831</td>\n",
       "      <td>0.317225</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.050375</td>\n",
       "      <td>0.184955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>0.018880</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.153966</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.090542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>-0.250185</td>\n",
       "      <td>-1.141559</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.023021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>-0.137269</td>\n",
       "      <td>-0.271806</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.036117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16652</th>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "      <td>0.010352</td>\n",
       "      <td>0.083313</td>\n",
       "      <td>0.514094</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.068266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16653 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_cells  highly_variable     means  dispersions  dispersions_norm  \\\n",
       "0            9            False  0.002611    -0.013101         -0.060134   \n",
       "1          570            False  0.135255    -0.001270          0.209814   \n",
       "2          225            False  0.050292    -0.021517          0.112911   \n",
       "3           12            False  0.001860    -0.149268         -0.466780   \n",
       "4          106            False  0.024858     0.030561          0.267766   \n",
       "...        ...              ...       ...          ...               ...   \n",
       "16648      322            False  0.070651     0.029831          0.317225   \n",
       "16649       78            False  0.018880     0.000764          0.153966   \n",
       "16650        9            False  0.001481    -0.250185         -1.141559   \n",
       "16651       15            False  0.003912    -0.137269         -0.271806   \n",
       "16652       38             True  0.010352     0.083313          0.514094   \n",
       "\n",
       "       highly_variable_nbatches  highly_variable_intersection      mean  \\\n",
       "0                             0                         False  0.001286   \n",
       "1                             1                         False  0.091207   \n",
       "2                             1                         False  0.034340   \n",
       "3                             0                         False  0.001729   \n",
       "4                             1                         False  0.016371   \n",
       "...                         ...                           ...       ...   \n",
       "16648                         2                         False  0.050375   \n",
       "16649                         2                         False  0.011738   \n",
       "16650                         0                         False  0.001020   \n",
       "16651                         0                         False  0.002027   \n",
       "16652                         3                         False  0.006145   \n",
       "\n",
       "            std  \n",
       "0      0.028724  \n",
       "1      0.246681  \n",
       "2      0.152490  \n",
       "3      0.033887  \n",
       "4      0.106600  \n",
       "...         ...  \n",
       "16648  0.184955  \n",
       "16649  0.090542  \n",
       "16650  0.023021  \n",
       "16651  0.036117  \n",
       "16652  0.068266  \n",
       "\n",
       "[16653 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09ca4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>size_factors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>748</td>\n",
       "      <td>0.607636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1052</td>\n",
       "      <td>0.854590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>739</td>\n",
       "      <td>0.600325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>874</td>\n",
       "      <td>0.709992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>951</td>\n",
       "      <td>0.772543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>6</td>\n",
       "      <td>1807</td>\n",
       "      <td>1.467912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>5</td>\n",
       "      <td>1249</td>\n",
       "      <td>1.014622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>7</td>\n",
       "      <td>2223</td>\n",
       "      <td>1.805849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>3</td>\n",
       "      <td>983</td>\n",
       "      <td>0.798538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>2</td>\n",
       "      <td>1211</td>\n",
       "      <td>0.983753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4271 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  n_genes  size_factors\n",
       "0        2      748      0.607636\n",
       "1        2     1052      0.854590\n",
       "2        2      739      0.600325\n",
       "3        8      874      0.709992\n",
       "4        3      951      0.772543\n",
       "...    ...      ...           ...\n",
       "4266     6     1807      1.467912\n",
       "4267     5     1249      1.014622\n",
       "4268     7     2223      1.805849\n",
       "4269     3      983      0.798538\n",
       "4270     2     1211      0.983753\n",
       "\n",
       "[4271 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6701e828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann.raw.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e453889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18050484,  2.43124879, -0.14809046, ...,  1.56127602,\n",
       "         0.09961133, -0.09001358],\n",
       "       [-0.18050484, -0.53800062, -0.14809046, ...,  1.69599056,\n",
       "         1.66225718, -0.09001358],\n",
       "       [-0.18050484, -0.53800062, -0.14809046, ...,  1.78981483,\n",
       "         1.59504406, -0.09001358],\n",
       "       ...,\n",
       "       [-0.18050484,  1.1276868 , -0.14809046, ...,  0.37132976,\n",
       "         0.64876498, -0.09001358],\n",
       "       [-0.18050484,  1.59388979, -0.14809046, ..., -0.255126  ,\n",
       "        -0.7171908 , -0.09001358],\n",
       "       [-0.18050484, -0.53800062, -0.14809046, ..., -0.74846542,\n",
       "         0.85199629, -0.09001358]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highly_variable_genes = data_ann.var[data_ann.var['highly_variable']].index.tolist()\n",
    "count_data_hvg = data_ann[:, highly_variable_genes].X\n",
    "count_data_hvg=count_data_hvg.toarray()\n",
    "count_data_hvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3942d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_count_hvg=data_ann.raw[:,highly_variable_genes].X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb7e32",
   "metadata": {},
   "source": [
    "## Create autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ada9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_binomial_loss(y_true, y_pred):\n",
    "    input_shape=int(y_pred.shape[1]/3)\n",
    "    mu = y_pred[:, :input_shape]\n",
    "    pi = y_pred[:, input_shape:input_shape*2]\n",
    "    theta = y_pred[:, input_shape*2:]\n",
    "    y_true = tf.cast(y_true, dtype='float32')\n",
    "    #print(type(mu), mu)\n",
    "    #print(type(pi),pi)\n",
    "    #print(type(theta), theta)\n",
    "    #print(type(y_true), y_true)\n",
    "\n",
    "\n",
    "    eps = 1e-10\n",
    "    t1 = tf.math.lgamma(theta+eps) + tf.math.lgamma(y_true+1.0) - tf.math.lgamma(y_true+theta+eps)\n",
    "    t2 = (theta+y_true) * tf.math.log(1.0 + (mu/(theta+eps))) + (y_true * (tf.math.log(theta+eps) - tf.math.log(mu+eps)))\n",
    "    final=t1 + t2\n",
    "    final = tf.reduce_mean(final)\n",
    "    return final\n",
    "\n",
    "def zero_inflated_negative_binomial_loss(y_true, y_pred):\n",
    "    input_shape=int(y_pred.shape[1]/3)\n",
    "    mu = y_pred[:, :input_shape]\n",
    "    pi = y_pred[:, input_shape:input_shape*2]\n",
    "    theta = y_pred[:, input_shape*2:]\n",
    "    y_true = tf.cast(y_true, dtype='float32')\n",
    "    #print(type(mu), mu)\n",
    "    #print(type(pi),pi)\n",
    "    #print(type(theta), theta)\n",
    "    #print(type(y_true), y_true)\n",
    "\n",
    "\n",
    "    eps = 1e-10\n",
    "    t1 = tf.math.lgamma(theta+eps) + tf.math.lgamma(y_true+1.0) - tf.math.lgamma(y_true+theta+eps)\n",
    "    t2 = (theta+y_true) * tf.math.log(1.0 + (mu/(theta+eps))) + (y_true * (tf.math.log(theta+eps) - tf.math.log(mu+eps)))\n",
    "    final=t1 + t2\n",
    "\n",
    "    nb_case = t1 + t2 - tf.math.log(1.0-pi+eps)\n",
    "    zero_nb = tf.pow(theta/(theta+mu+eps), theta)\n",
    "    zero_case = -tf.math.log(pi + ((1.0-pi)*zero_nb)+eps)\n",
    "    result = tf.where(tf.less(y_true, 1e-8), zero_case, nb_case)\n",
    "    #ridge = self.ridge_lambda*tf.square(self.pi)\n",
    "    #result += ridge\n",
    "    result = tf.reduce_mean(result)\n",
    "    return result\n",
    "\n",
    "MeanAct = lambda x: tf.clip_by_value(tf.keras.backend.exp(x), 1e-5, 1e6)\n",
    "DispAct = lambda x: tf.clip_by_value(tf.keras.backend.softplus(x), 1e-4, 1e4)\n",
    "\n",
    "ColWiseMultLayer = lambda name: layers.Lambda(lambda l: l[0]*(tf.matmul(tf.reshape(l[1], (-1,1)),\n",
    "                                                                 tf.ones((1, l[0].get_shape()[1]),\n",
    "                                                                         dtype=l[1].dtype))), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4008cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters=n_clusters\n",
    "        self.alpha=alpha\n",
    "        self.intial_weights=weights\n",
    "        #self.input_spec=keras.InputSpec(ndim=2) #to specify the expected rank of the input\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim=input_shape[1]\n",
    "        #self.input_spec=keras.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform')\n",
    "        if self.intial_weights is not None :\n",
    "            self.set_weights(self.intial_weights)\n",
    "            del self.intial_weights\n",
    "        self.built=True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        q = 1.0 / (1.0 + (tf.math.reduce_sum(tf.math.square(tf.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = tf.transpose(tf.transpose(q) / tf.math.reduce_sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "599b945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_shape, noise):\n",
    "    init='glorot_uniform'\n",
    "    Inputs = layers.Input(shape=(input_shape,), name='Inputs')\n",
    "    sf_layer=layers.Input(shape=(1,), name=\"size_factors\")\n",
    "    x=layers.GaussianNoise(noise)(Inputs)\n",
    "    x=layers.Dense(256, activation='relu',kernel_initializer=init, name='encoder_1' )(x)\n",
    "    x=layers.GaussianNoise(noise)(x)\n",
    "    x=layers.Dense(64, activation='relu',kernel_initializer=init, name='encoder_2' )(x)\n",
    "    x=layers.GaussianNoise(noise)(x)\n",
    "    hidden=layers.Dense(32, activation='relu',kernel_initializer=init, name='encoder_3' )(x)\n",
    "\n",
    "\n",
    "    x=layers.Dense(32, activation='relu',kernel_initializer=init, name='decoder_1' )(hidden)\n",
    "    x=layers.Dense(64, activation='relu',kernel_initializer=init, name='decoder_2' )(x)\n",
    "    x=layers.Dense(256, activation='relu',kernel_initializer=init, name='decoder_3' )(x)\n",
    "    pi=layers.Dense(input_shape, activation=\"sigmoid\",kernel_initializer=init, name='pi')(x)\n",
    "    disp=layers.Dense(input_shape, activation=DispAct,kernel_initializer=init, name='dispersion')(x)\n",
    "    mean=layers.Dense(input_shape, activation=MeanAct,kernel_initializer=init, name='mean')(x)\n",
    "\n",
    "    Outputs=ColWiseMultLayer(name='outputs')([mean, sf_layer])\n",
    "    #Outputs=SliceLayer(0, name='slice')([Outputs, disp, pi])\n",
    "    outputs = layers.Concatenate(axis=1, name='output')([Outputs, pi, disp])\n",
    "\n",
    "    autoencoder=Model([Inputs, sf_layer], outputs, name='autoencoder_ZINB')\n",
    "    autoencoder.compile(optimizer='adam', loss={'output': zero_inflated_negative_binomial_loss})\n",
    "\n",
    "    autoencoder.summary()\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6e8f865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16653"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape=data_ann.n_vars\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d774f3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16653"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ann.X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93003b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs  n_vars = 4271  16653\n",
       "    obs: 'label', 'n_genes', 'size_factors'\n",
       "    var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection', 'mean', 'std'\n",
       "    uns: 'log1p', 'hvg'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_ann.X)\n",
    "data_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4723f50",
   "metadata": {},
   "source": [
    "On entrane le modle sur l'ensemble des donnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea15ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_latent_space(y_pred, model, count_data_hvg , size_factors, obs):\n",
    "    encoder= Model(inputs=model.input, outputs=model.get_layer(\"encoder_3\").output)\n",
    "    encoder.summary()\n",
    "    predict_data=encoder.predict([count_data_hvg, size_factors])\n",
    "    adata_latent = sc.AnnData(predict_data)\n",
    "    adata_latent.obs=obs\n",
    "    adata_latent.obs['predict']=y_pred\n",
    "    anno=adata_latent.obs['label']\n",
    "    sc.pp.neighbors(adata_latent)\n",
    "    sc.tl.umap(adata_latent)\n",
    "    sc.pl.umap(adata_latent, color=\"label\")\n",
    "    sc.pl.umap(adata_latent, color=\"predict\")\n",
    "    crosstab = pd.crosstab(y_pred,anno)\n",
    "    sns.heatmap(crosstab, annot=True, cmap='Blues')\n",
    "    plt.ylabel('Clusters prdits')\n",
    "    plt.xlabel('Annotations relles')\n",
    "    plt.title('Matrice de confusion')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfbe54b7-e60e-42da-b27b-33c757a87355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "   \n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    # Assurez-vous que les tiquettes sont de type str\n",
    "    y_true= y_true.astype(str)\n",
    "    y_pred = y_pred.astype(str)\n",
    "    \n",
    "    # Trouver les tiquettes uniques\n",
    "    labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    # Construire la matrice de cot (matrice de confusion)\n",
    "    cost_matrix = np.zeros((n_labels, n_labels), dtype=int)\n",
    "    for i, label_true in enumerate(labels):\n",
    "        for j, label_pred in enumerate(labels):\n",
    "            cost_matrix[i, j] = np.sum((y_true == label_true) & (y_pred == label_pred))\n",
    "\n",
    "    # Rsoudre le problme de correspondance bipartite optimal\n",
    "    row_ind, col_ind = linear_assignment(cost_matrix.max() - cost_matrix)\n",
    "\n",
    "    # Calculer la prcision\n",
    "    accuracy = np.sum([cost_matrix[i, j] for i, j in zip(row_ind, col_ind)]) / y_true.size\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cced50e-ebb3-47e8-bec1-cde59122b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(history, filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, mode='a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(history.keys())\n",
    "        writer.writerow(history.values())\n",
    "        \n",
    "def check_existing_filename(filename):\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    while os.path.exists(filename):\n",
    "        filename = f\"{base}_{counter}{ext}\"\n",
    "        counter += 1\n",
    "    return filename\n",
    "\n",
    "def save_plot_umap(model, x, size_factors, y, y_pred, res, iteration, pdf_pages, train_test=\"train\"):\n",
    "    #Rcupration des donnes et projection dans l'espace latent \n",
    "    encoder= Model(inputs=model.input, outputs=model.get_layer(\"encoder_3\").output)\n",
    "    predict_data=encoder.predict([x, size_factors], verbose=0)\n",
    "    obs_df = pd.DataFrame({'label': y})\n",
    "    \n",
    "    #Prparation des donnes pour Scanpy\n",
    "    adata_latent = sc.AnnData(X=predict_data)\n",
    "    adata_latent.obs = obs_df\n",
    "    adata_latent.obs['predict'] = y_pred.astype(str)\n",
    "\n",
    "    sc.pp.neighbors(adata_latent)\n",
    "    sc.tl.umap(adata_latent)\n",
    "    \n",
    "    # Gnration du UMAP avec Scanpy\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    sc.pl.umap(adata_latent, color='label', ax=axs[0], show=False)\n",
    "    axs[0].set_title(f'UMAP projection - Labels ({train_test}) (Res: {res}, Iter: {iteration})')\n",
    "    sc.pl.umap(adata_latent, color='predict', ax=axs[1], show=False)\n",
    "    axs[1].set_title(f'UMAP projection - Predictions ({train_test}) (Res: {res}, Iter: {iteration})')\n",
    "    \n",
    "\n",
    "    pdf_pages.savefig(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19122365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q**2/q.sum(0)\n",
    "    return (weight.T/weight.sum(1)).T\n",
    "\n",
    "def auto_kmeans(encoder, x_counts, size_factors, obs, plot=False):\n",
    "    y=obs[\"label\"]\n",
    "    ari=[]\n",
    "    nmi=[]\n",
    "    x=[]\n",
    "    for n in range (1,20):\n",
    "        kmeans=KMeans(n_clusters=n, n_init=30, verbose=0)\n",
    "        y_pred=kmeans.fit_predict(encoder.predict([x_counts, size_factors]))\n",
    "        ari.append(adjusted_rand_score(y, y_pred))\n",
    "        nmi.append(normalized_mutual_info_score(y, y_pred))\n",
    "        x.append(n)\n",
    "    somme_metriques = [x + y for x, y in zip(ari, nmi)]\n",
    "    n_max=(somme_metriques.index(max(somme_metriques))+1)\n",
    "    \n",
    "    kmeans=KMeans(n_clusters=n_max, n_init=20)\n",
    "    y_pred=kmeans.fit_predict(encoder.predict([x_counts, size_factors]))\n",
    "    \n",
    "    if plot==True:\n",
    "        predict_data=encoder.predict([x_counts, size_factors])\n",
    "        adata_latent = sc.AnnData(predict_data)\n",
    "        adata_latent.obs=obs\n",
    "        adata_latent.obs[\"kmeans\"]=y_pred\n",
    "        sc.pp.neighbors(adata_latent)\n",
    "        sc.tl.umap(adata_latent)\n",
    "        sc.pl.umap(adata_latent, color='label')\n",
    "        plt.plot(x,ari)\n",
    "        plt.xlabel(\"nombre de clusters\")\n",
    "        plt.ylabel(\"ARI\")\n",
    "        plt.plot(x,nmi)\n",
    "        plt.xlabel(\"nombre de clusters\")\n",
    "        plt.ylabel(\"NMI\")\n",
    "        ari = adjusted_rand_score(y, y_pred)\n",
    "        print(\"Indice de Rand ajust (ARI) :\", ari)\n",
    "        nmi = normalized_mutual_info_score(y, y_pred)\n",
    "        print(\"Normalized mutual info (NMI) :\", nmi)\n",
    "        sc.pl.umap( adata_latent, color=[\"kmeans\"], legend_loc=\"on data\")\n",
    "        plt.plot()\n",
    "    return y_pred, n_max, kmeans.cluster_centers_\n",
    "\n",
    "def auto_leiden(encoder, x_counts, size_factors, y, res=\"auto\", plot=False):\n",
    "    predict_data=encoder.predict([x_counts, size_factors], verbose=0)\n",
    "    adata_latent = sc.AnnData(predict_data)\n",
    "    obs_df = pd.DataFrame({'label': y})\n",
    "    adata_latent.obs=obs_df\n",
    "    sc.pp.neighbors(adata_latent)\n",
    "    sc.tl.umap(adata_latent)\n",
    "    list_ari=[]\n",
    "    list_nmi=[]\n",
    "    x=[]\n",
    "    \n",
    "    if res==\"auto\":\n",
    "        #search for the best resolution\n",
    "        for i in range (1,10):\n",
    "            sc.tl.leiden(adata_latent, key_added=\"leiden\", resolution=i/100)\n",
    "            predict_cluster=adata_latent.obs[\"leiden\"]\n",
    "            list_ari.append(adjusted_rand_score(y, predict_cluster))\n",
    "            list_nmi.append(normalized_mutual_info_score(y, predict_cluster))\n",
    "            x.append(i/100)\n",
    "        for i in range (1,11):\n",
    "            sc.tl.leiden(adata_latent, key_added=\"leiden\", resolution=i/10)\n",
    "            predict_cluster=adata_latent.obs[\"leiden\"]\n",
    "            list_ari.append(adjusted_rand_score(y, predict_cluster))\n",
    "            list_nmi.append(normalized_mutual_info_score(y, predict_cluster))\n",
    "            x.append(i/10)\n",
    "        somme_metriques = [x + y for x, y in zip(list_ari, list_nmi)]\n",
    "        res=x[somme_metriques.index(max(somme_metriques))]\n",
    "        print(\"La rsolution est de : \", res)\n",
    "    #compute for the best resolution\n",
    "    sc.tl.leiden(adata_latent, key_added=\"leiden_res_%.4f\" % (res), resolution=res)\n",
    "    predict=adata_latent.obs[\"leiden_res_%.4f\" % (res)]\n",
    "    \n",
    "    \n",
    "    #compute cluster center for initialization\n",
    "    init_pred=np.asarray(predict,dtype=int)\n",
    "    features=pd.DataFrame(adata_latent.X,index=np.arange(0,adata_latent.shape[0]))\n",
    "    Group=pd.Series(init_pred,index=np.arange(0,adata_latent.shape[0]),name=\"Group\")\n",
    "    Mergefeature=pd.concat([features,Group],axis=1)\n",
    "    cluster_centers=np.asarray(Mergefeature.groupby(\"Group\").mean())\n",
    "    n_clusters=len(np.unique(init_pred))\n",
    "    \n",
    "    #set of plot if required\n",
    "    if plot==True:\n",
    "        sc.pl.umap(adata_latent, color='label')\n",
    "        plt.plot(x,list_ari)\n",
    "        plt.xlabel(\"nombre de clusters\")\n",
    "        plt.ylabel(\"ARI\")\n",
    "        plt.plot(x,list_nmi)\n",
    "        plt.xlabel(\"nombre de clusters\")\n",
    "        plt.ylabel(\"NMI\")\n",
    "        sc.pl.umap( adata_latent, color=[\"leiden_res_%.4f\" % (res)], legend_loc=\"on data\")\n",
    "        ari = adjusted_rand_score(y, predict)\n",
    "        print(\"Indice de Rand ajust (ARI) :\", ari)\n",
    "        nmi = normalized_mutual_info_score(y, predict)\n",
    "        print(\"Normalized mutual info (NMI) :\", nmi)\n",
    "        plt.plot()\n",
    "        #crosstab = pd.crosstab(predict,y)\n",
    "        #sns.heatmap(crosstab, annot=True, cmap='Blues')\n",
    "        #plt.ylabel('Clusters prdits')\n",
    "        #plt.xlabel('Annotations relles')\n",
    "        #plt.title('Matrice de confusion')\n",
    "        #plt.show()\n",
    "    return res, predict, n_clusters, cluster_centers\n",
    "    \n",
    "def split(x_counts, raw_counts, size_factors, y):\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(y)), stratify=y, test_size=0.2, random_state=42)\n",
    "    x_train=x_counts[train_idx]\n",
    "    x_test=x_counts[test_idx]\n",
    "    size_factors_train=size_factors[train_idx]\n",
    "    size_factors_test=size_factors[test_idx]\n",
    "    raw_train=raw_counts[train_idx]\n",
    "    raw_test=raw_counts[test_idx]\n",
    "    y_train=y[train_idx]\n",
    "    y_test=y[test_idx]\n",
    "    print(\"Size of train set : \", x_train.shape)\n",
    "    print(\"Size of test set : \", x_test.shape)\n",
    "    return x_train, x_test, size_factors_train, size_factors_test, raw_train, raw_test, y_train, y_test\n",
    "    \n",
    "def fit_and_split(x_counts, obs, size_factors, raw_counts, alpha, n_cluster=\"auto\", res=\"auto\", method=\"leiden\", noise=0.5, batch_size=256,\n",
    "        max_iter=2e4, tol=1e-3, update_interval=140, loss_weights=[1,1],\n",
    "        ae_weights=None, pretrained=False):\n",
    "    print('Update interval', update_interval)\n",
    "    t0 = time.time()\n",
    "    y=obs[\"label\"]\n",
    "    #step 0 split data\n",
    "    x_train, x_test, size_factors_train, size_factors_test, raw_train, raw_test, y_train, y_test=split(x_counts, raw_counts, size_factors, y)\n",
    "    input_shape=x_train.shape[1]\n",
    "    #Step 1 Pretrain \n",
    "    if pretrained==False or ae_weights is None :\n",
    "        print(\"..pretraining autoencoder : \")\n",
    "        autoencoder=create_autoencoder(input_shape, noise)\n",
    "        callback= tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, mode='min', verbose=1)\n",
    "        autoencoder.fit(x=[x_train, size_factors_train], y=raw_train, validation_data=([x_test,size_factors_test], raw_test), batch_size=batch_size, epochs=200, callbacks=[callback])\n",
    "        autoencoder.save_weights(\"./model/weights/dif_pbmc_ae_res.weights.h5\")\n",
    "        ae_weights=\"./model/weights/dif_pbmc_ae_res.weights.h5\"\n",
    "    elif ae_weights is not None:\n",
    "            autoencoder=create_autoencoder(input_shape, noise)\n",
    "            autoencoder.load_weights(ae_weights)\n",
    "            print('ae_weights is loaded successfully.')\n",
    "    \n",
    "    ae_layers = [l for l in autoencoder.layers]\n",
    "    hidden = autoencoder.input[0]\n",
    "    for i in range(1, len(ae_layers)):\n",
    "        if \"noise\" in ae_layers[i].name:\n",
    "            next\n",
    "        elif \"dropout\" in ae_layers[i].name:\n",
    "            next\n",
    "        else:\n",
    "            hidden = ae_layers[i](hidden)\n",
    "        if \"encoder_3\" in ae_layers[i].name:  # only get encoder layers\n",
    "             break\n",
    "    encoder = Model(inputs=autoencoder.input, outputs=hidden, name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    \n",
    "    #step 2 intialize clusters:\n",
    "    \n",
    "    #rcuprons l'autoencoder\n",
    "    #encoder= Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(\"hidden\").output)\n",
    "    if method==\"leiden\":\n",
    "        print(\"Initializing cluster centers with leiden : \")\n",
    "        res, y_pred, n_cluster, cluster_centers = auto_leiden(encoder, x_train, size_factors_train, y_train, res=res, plot=False)\n",
    "        print('Le nombre de clusters est : ', n_cluster)\n",
    "    elif method==\"kmeans\":\n",
    "        print(\"Initializing cluster centers with k-means : \")\n",
    "        if n_cluster==\"auto\":\n",
    "            y_pred, n_cluster, cluster_centers=auto_kmeans(encoder, x_train, size_factors_train, obs, plot=False)\n",
    "            print('Le nombre de clusters est : ', n_cluster)\n",
    "        else :\n",
    "            kmeans=KMeans(n_clusters=n_cluster, n_init=20)\n",
    "            y_pred=kmeans.fit_predict(encoder.predict([x_train, size_factors]))\n",
    "            cluster_centers=kmeans.cluster_centers_\n",
    "        \n",
    "    y_pred_last_train=np.copy(y_pred)\n",
    "    \n",
    "    clustering_layer = ClusteringLayer(n_cluster, alpha=alpha, name='clustering')(hidden)\n",
    "    model= Model(inputs=[autoencoder.input[0], autoencoder.input[1]],\n",
    "                           outputs=[clustering_layer, autoencoder.output])\n",
    "    model.summary()\n",
    "    model.compile(loss={'clustering': KLDivergence, 'output': zero_inflated_negative_binomial_loss}, optimizer='adam', metrics={'clustering': KLDivergence, 'output': zero_inflated_negative_binomial_loss})\n",
    "    print(\"Set clustering weights\")\n",
    "    model.get_layer(name='clustering').set_weights([cluster_centers])\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #step 3 deep clustering\n",
    "    print(\"..Starting Deep Clustering\")\n",
    "    loss=[0,0,0]\n",
    "    val_loss=[0,0,0]\n",
    "    index=0\n",
    "    save_interval = int(x_counts.shape[0] / batch_size) * 5 \n",
    "    \n",
    "    #Cration du dictionnaire pour le monitoring\n",
    "    history={\n",
    "        \"res\":res,\n",
    "        \"clusters\": n_cluster,\n",
    "        \"weight\": loss_weights[0],\n",
    "        \"NMI\" :[],\n",
    "        \"ARI\" :[],\n",
    "        \"CA\" :[],\n",
    "        \"val_NMI\":[],\n",
    "        \"val_ARI\":[],\n",
    "        \"val_CA\":[],\n",
    "        \"loss\":[],\n",
    "        \"val_loss\":[],\n",
    "        \"clustering_loss\":[],\n",
    "        \"val_clustering_loss\":[],\n",
    "        \"zinb_loss\":[],\n",
    "        \"val_zinb_loss\":[]\n",
    "    }\n",
    "        \n",
    "    #initiate pdf files\n",
    "    pdf_filename_train = 'data/res_test/plots/pbmc/plots_all_train_res_%.4f.pdf' % res\n",
    "    #pdf_filename_train = check_existing_filename(pdf_filename_train)\n",
    "    pdf_pages_train = PdfPages(pdf_filename_train)\n",
    "    \n",
    "    pdf_filename_test = 'data/res_test/plots/pbmc/plots_all_test_res_%.4f.pdf' % res\n",
    "    #pdf_filename_test = check_existing_filename(pdf_filename_test)\n",
    "    pdf_pages_test = PdfPages(pdf_filename_test)\n",
    "    \n",
    "    for iteration in range(int(max_iter)):\n",
    "       \n",
    "        #if iteration % update_interval==0:\n",
    "        \n",
    "        if index==0:\n",
    "            q_train,_= model.predict([x_train, size_factors_train], verbose=0)\n",
    "            p_train=target_distribution(q_train)\n",
    "            q_test,_= model.predict([x_test, size_factors_test], verbose=0)\n",
    "            p_test=target_distribution(q_test)\n",
    "            \n",
    "            loss=model.evaluate(x=[x_train, size_factors_train], y=[p_train, raw_train], batch_size=batch_size, verbose=0)\n",
    "            val_loss=model.evaluate(x=[x_test, size_factors_test], y=[p_test, raw_test], batch_size=batch_size, verbose=0)\n",
    "            \n",
    "            y_pred_train=q_train.argmax(1)\n",
    "            y_pred_test=q_test.argmax(1)\n",
    "            if y is not None :\n",
    "                ca=np.round(cluster_acc(y_train, y_pred_train), 5)\n",
    "                nmi=np.round(normalized_mutual_info_score(y_train, y_pred_train), 5)\n",
    "                ari=np.round(adjusted_rand_score(y_train, y_pred_train), 5)\n",
    "                val_ca=np.round(cluster_acc(y_test, y_pred_test), 5)\n",
    "                val_nmi=np.round(normalized_mutual_info_score(y_test, y_pred_test), 5)\n",
    "                val_ari=np.round(adjusted_rand_score(y_test, y_pred_test), 5)\n",
    "                print('Iter-%d: CA=%.4f, NMI= %.4f, ARI= %.4f; L= %.5f, Lc= %.5f,  Lr= %.5f'\n",
    "                          % (iteration, ca, nmi, ari, loss[0], loss[1], loss[2]))\n",
    "                print('CA=%.4f, val_NMI= %.4f, val_ARI= %.4f; val_L= %.5f, val_Lc= %.5f,  val_Lr= %.5f'\n",
    "                          % (val_ca, val_nmi, val_ari, val_loss[0], val_loss[1], val_loss[2]))\n",
    "                \n",
    "                #maj du dictionnaire \n",
    "                history[\"CA\"].append(ca)\n",
    "                history[\"NMI\"].append(nmi)\n",
    "                history[\"ARI\"].append(ari)\n",
    "                history[\"val_CA\"].append(val_ca)\n",
    "                history[\"val_NMI\"].append(val_nmi)\n",
    "                history[\"val_ARI\"].append(val_ari)\n",
    "                history[\"loss\"].append(loss[0])\n",
    "                history[\"clustering_loss\"].append(loss[1])\n",
    "                history[\"zinb_loss\"].append(loss[2])\n",
    "                history[\"val_loss\"].append(val_loss[0])\n",
    "                history[\"val_clustering_loss\"].append(val_loss[1])\n",
    "                history[\"val_zinb_loss\"].append(val_loss[2])\n",
    "                \n",
    "                if iteration==0:\n",
    "                    history['CA_initial']=ca\n",
    "                    history['ARI_initial']=ari\n",
    "                    history['NMI_initial']=nmi\n",
    "                    history['val_CA_initial']=val_ca\n",
    "                    history['val_ARI_initial']=val_ari\n",
    "                    history['val_NMI_initial']=val_nmi\n",
    "                    \n",
    "            \n",
    "            #save a plot\n",
    "            save_plot_umap(model, x_train, size_factors_train, y_train, y_pred_train, res, iteration, pdf_pages_train, \"train\")\n",
    "            save_plot_umap(model, x_test, size_factors_test, y_test, y_pred_test, res, iteration, pdf_pages_test, \"test\")\n",
    "\n",
    "\n",
    "                \n",
    "            #stop criterion\n",
    "            delta_label=np.sum(y_pred_train != y_pred_last_train).astype(np.float32)/y_pred_train.shape[0]\n",
    "            y_pred_last_train=np.copy(y_pred_train)\n",
    "            if iteration >0 and delta_label<tol:\n",
    "                print('delta_label ', delta_label, '< tol ', tol)\n",
    "                print('Reached tolerance threshold. Stopping training.')\n",
    "                break\n",
    "    \n",
    "        if (index + 1)*batch_size > x_train.shape[0]:\n",
    "            model.train_on_batch(x=[x_train[index * batch_size::], size_factors_train[index * batch_size:]],\n",
    "                                                 y=[p_train[index * batch_size::], raw_train[index * batch_size::]])\n",
    "            index=0\n",
    "        else:\n",
    "            model.train_on_batch(x=[x_train[index * batch_size:(index + 1) * batch_size], \n",
    "                                                    size_factors_train[index * batch_size:(index + 1) * batch_size]],\n",
    "                                                 y=[p_train[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    raw_train[index * batch_size:(index + 1) * batch_size]])\n",
    "            index += 1\n",
    "        \n",
    "        #if iteration % save_interval == 0:\n",
    "            # save scDeepCluster model checkpoints\n",
    "            #print('saving model to: ''/weights' + str(iteration) + '.h5')\n",
    "            #model.save_weights('/weights' + str(iteration) + '.h5')\n",
    "            #print('saving model to: model/weights.weights.h5')\n",
    "            #model.save_weights('model/weights.weights.h5')\n",
    "    \n",
    "        #iteration+=1\n",
    "    \n",
    "    ca = np.round(cluster_acc(y_train, y_pred_train), 5)\n",
    "    nmi = np.round(normalized_mutual_info_score(y_train, y_pred_train), 5)\n",
    "    ari = np.round(adjusted_rand_score(y_train, y_pred_train), 5)\n",
    "    val_ca = np.round(cluster_acc(y_test, y_pred_test), 5)\n",
    "    val_nmi = np.round(normalized_mutual_info_score(y_test, y_pred_test), 5)\n",
    "    val_ari = np.round(adjusted_rand_score(y_test, y_pred_test), 5)\n",
    "    print('Final: CA=%.4f, NMI= %.4f, ARI= %.4f' % (ca, nmi, ari))\n",
    "    print('Final: val_CA=%.4f, val_NMI= %.4f, val_ARI= %.4f' % (val_ca, val_nmi, val_ari))\n",
    "    duration=int(time.time() - t0)\n",
    "    print('Clustering time: %d seconds.' % duration )\n",
    "    history[\"training_time\"]=duration\n",
    "    history[\"nbr_iteration\"]=iteration\n",
    "    history['CA_final']=ca\n",
    "    history['ARI_final']=ari\n",
    "    history['NMI_final']=nmi\n",
    "    history['val_CA_final']=val_ca\n",
    "    history['val_ARI_final']=val_ari\n",
    "    history['val_NMI_final']=val_nmi\n",
    "                    \n",
    "    \n",
    "    #save dictionnary\n",
    "    save_results_to_csv(history, 'data/res_test/results_all_pbmc_diff_AE_res_2.csv')\n",
    "    \n",
    "    pdf_pages_train.close()\n",
    "    pdf_pages_test.close()\n",
    "    \n",
    "    return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93969a-85e3-4abe-b679-73453e9f9b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 27\n",
      "Size of train set :  (3416, 16653)\n",
      "Size of test set :  (855, 16653)\n",
      "..pretraining autoencoder : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3781464/301523525.py:112: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factors_train=size_factors[train_idx]\n",
      "/tmp/ipykernel_3781464/301523525.py:113: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factors_test=size_factors[test_idx]\n",
      "/tmp/ipykernel_3781464/301523525.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train=y[train_idx]\n",
      "/tmp/ipykernel_3781464/301523525.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test=y[test_idx]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"autoencoder_ZINB\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"autoencoder_ZINB\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " Inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       "\n",
       " gaussian_noise       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  Inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)                                                       \n",
       "\n",
       " encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,263,424</span>  gaussian_noise[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " gaussian_noise_1     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)                                                       \n",
       "\n",
       " encoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  gaussian_noise_1 \n",
       "\n",
       " gaussian_noise_2     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  encoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)                                                       \n",
       "\n",
       " encoder_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  gaussian_noise_2 \n",
       "\n",
       " decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span>  encoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " decoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span>  decoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " decoder_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  decoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,279,821</span>  decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " size_factors         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                     size_factors[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " pi (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,279,821</span>  decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " dispersion (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,279,821</span>  decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " output               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49959</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  outputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       pi[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
       "                                                     dispersion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " Inputs (\u001b[38;5;33mInputLayer\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       "\n",
       " gaussian_noise       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)               \u001b[38;5;34m0\u001b[0m  Inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
       " (\u001b[38;5;33mGaussianNoise\u001b[0m)                                                       \n",
       "\n",
       " encoder_1 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         \u001b[38;5;34m4,263,424\u001b[0m  gaussian_noise[\u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " gaussian_noise_1     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  encoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mGaussianNoise\u001b[0m)                                                       \n",
       "\n",
       " encoder_2 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m16,448\u001b[0m  gaussian_noise_1 \n",
       "\n",
       " gaussian_noise_2     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  encoder_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mGaussianNoise\u001b[0m)                                                       \n",
       "\n",
       " encoder_3 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m2,080\u001b[0m  gaussian_noise_2 \n",
       "\n",
       " decoder_1 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m1,056\u001b[0m  encoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " decoder_2 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m2,112\u001b[0m  decoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " decoder_3 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m16,640\u001b[0m  decoder_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " mean (\u001b[38;5;33mDense\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)       \u001b[38;5;34m4,279,821\u001b[0m  decoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " size_factors         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " outputs (\u001b[38;5;33mLambda\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)               \u001b[38;5;34m0\u001b[0m  mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                     size_factors[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " pi (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)       \u001b[38;5;34m4,279,821\u001b[0m  decoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " dispersion (\u001b[38;5;33mDense\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)       \u001b[38;5;34m4,279,821\u001b[0m  decoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " output               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49959\u001b[0m)               \u001b[38;5;34m0\u001b[0m  outputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n",
       " (\u001b[38;5;33mConcatenate\u001b[0m)                                       pi[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
       "                                                     dispersion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,141,223</span> (65.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,141,223\u001b[0m (65.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,141,223</span> (65.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,141,223\u001b[0m (65.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 729ms/step - loss: 0.4660 - val_loss: 0.2866\n",
      "Epoch 2/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 722ms/step - loss: 0.2861 - val_loss: 0.2626\n",
      "Epoch 3/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 710ms/step - loss: 0.2561 - val_loss: 0.2502\n",
      "Epoch 4/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 713ms/step - loss: 0.2455 - val_loss: 0.2472\n",
      "Epoch 5/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 721ms/step - loss: 0.2432 - val_loss: 0.2452\n",
      "Epoch 6/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 724ms/step - loss: 0.2408 - val_loss: 0.2451\n",
      "Epoch 7/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 720ms/step - loss: 0.2388 - val_loss: 0.2436\n",
      "Epoch 8/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 748ms/step - loss: 0.2376 - val_loss: 0.2440\n",
      "Epoch 9/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 717ms/step - loss: 0.2373 - val_loss: 0.2443\n",
      "Epoch 10/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 720ms/step - loss: 0.2384 - val_loss: 0.2442\n",
      "Epoch 11/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 732ms/step - loss: 0.2379 - val_loss: 0.2427\n",
      "Epoch 12/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 719ms/step - loss: 0.2344 - val_loss: 0.2449\n",
      "Epoch 13/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 753ms/step - loss: 0.2357 - val_loss: 0.2431\n",
      "Epoch 14/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 724ms/step - loss: 0.2363 - val_loss: 0.2437\n",
      "Epoch 15/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 856ms/step - loss: 0.2335 - val_loss: 0.2434\n",
      "Epoch 16/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 721ms/step - loss: 0.2340 - val_loss: 0.2440\n",
      "Epoch 17/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 723ms/step - loss: 0.2340 - val_loss: 0.2434\n",
      "Epoch 18/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 715ms/step - loss: 0.2345 - val_loss: 0.2435\n",
      "Epoch 19/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 715ms/step - loss: 0.2341 - val_loss: 0.2450\n",
      "Epoch 20/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 712ms/step - loss: 0.2340 - val_loss: 0.2424\n",
      "Epoch 21/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 713ms/step - loss: 0.2333 - val_loss: 0.2436\n",
      "Epoch 22/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 728ms/step - loss: 0.2325 - val_loss: 0.2422\n",
      "Epoch 23/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 726ms/step - loss: 0.2316 - val_loss: 0.2429\n",
      "Epoch 24/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 717ms/step - loss: 0.2315 - val_loss: 0.2435\n",
      "Epoch 25/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 727ms/step - loss: 0.2304 - val_loss: 0.2435\n",
      "Epoch 26/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 715ms/step - loss: 0.2304 - val_loss: 0.2433\n",
      "Epoch 27/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 713ms/step - loss: 0.2319 - val_loss: 0.2443\n",
      "Epoch 28/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 717ms/step - loss: 0.2289 - val_loss: 0.2450\n",
      "Epoch 29/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 755ms/step - loss: 0.2305 - val_loss: 0.2431\n",
      "Epoch 30/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 714ms/step - loss: 0.2303 - val_loss: 0.2435\n",
      "Epoch 31/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 786ms/step - loss: 0.2273 - val_loss: 0.2430\n",
      "Epoch 32/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 726ms/step - loss: 0.2297 - val_loss: 0.2447\n",
      "Epoch 32: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " Inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       "\n",
       " encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,263,424</span>  Inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
       "\n",
       " encoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " size_factors         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " encoder_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  encoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " Inputs (\u001b[38;5;33mInputLayer\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       "\n",
       " encoder_1 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         \u001b[38;5;34m4,263,424\u001b[0m  Inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
       "\n",
       " encoder_2 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m16,448\u001b[0m  encoder_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " size_factors         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " encoder_3 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m2,080\u001b[0m  encoder_2[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,281,952</span> (16.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,281,952\u001b[0m (16.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,281,952</span> (16.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,281,952\u001b[0m (16.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with leiden : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3781464/301523525.py:73: FutureWarning: In the future, the default backend for leiden will be igraph instead of leidenalg.\n",
      "\n",
      " To achieve the future defaults please pass: flavor=\"igraph\" and n_iterations=2.  directed must also be False to work with igraph's implementation.\n",
      "  sc.tl.leiden(adata_latent, key_added=\"leiden_res_%.4f\" % (res), resolution=res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de clusters est :  13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " Inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       "\n",
       " gaussian_noise       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  Inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)                                                       \n",
       "\n",
       " encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,263,424</span>  gaussian_noise[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "                                                     Inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
       "\n",
       " gaussian_noise_1     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)                                                       \n",
       "\n",
       " encoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  gaussian_noise_1 \n",
       "                                                     encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " gaussian_noise_2     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  encoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GaussianNoise</span>)                                                       \n",
       "\n",
       " encoder_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  gaussian_noise_2 \n",
       "                                                     encoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span>  encoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " decoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span>  decoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " decoder_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  decoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,279,821</span>  decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " size_factors         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                     size_factors[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       " pi (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,279,821</span>  decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " dispersion (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16653</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,279,821</span>  decoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       " clustering           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>  encoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ClusteringLayer</span>)                                                     \n",
       "\n",
       " output               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49959</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  outputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       pi[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
       "                                                     dispersion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " Inputs (\u001b[38;5;33mInputLayer\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)               \u001b[38;5;34m0\u001b[0m  -                 \n",
       "\n",
       " gaussian_noise       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)               \u001b[38;5;34m0\u001b[0m  Inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
       " (\u001b[38;5;33mGaussianNoise\u001b[0m)                                                       \n",
       "\n",
       " encoder_1 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         \u001b[38;5;34m4,263,424\u001b[0m  gaussian_noise[\u001b[38;5;34m0\u001b[0m \n",
       "                                                     Inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
       "\n",
       " gaussian_noise_1     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  encoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mGaussianNoise\u001b[0m)                                                       \n",
       "\n",
       " encoder_2 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m16,448\u001b[0m  gaussian_noise_1 \n",
       "                                                     encoder_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " gaussian_noise_2     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  encoder_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mGaussianNoise\u001b[0m)                                                       \n",
       "\n",
       " encoder_3 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m2,080\u001b[0m  gaussian_noise_2 \n",
       "                                                     encoder_2[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " decoder_1 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m1,056\u001b[0m  encoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " decoder_2 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m2,112\u001b[0m  decoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " decoder_3 (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m16,640\u001b[0m  decoder_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " mean (\u001b[38;5;33mDense\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)       \u001b[38;5;34m4,279,821\u001b[0m  decoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " size_factors         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " outputs (\u001b[38;5;33mLambda\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)               \u001b[38;5;34m0\u001b[0m  mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                     size_factors[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
       "\n",
       " pi (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)       \u001b[38;5;34m4,279,821\u001b[0m  decoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " dispersion (\u001b[38;5;33mDense\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16653\u001b[0m)       \u001b[38;5;34m4,279,821\u001b[0m  decoder_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n",
       " clustering           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)                \u001b[38;5;34m416\u001b[0m  encoder_3[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       " (\u001b[38;5;33mClusteringLayer\u001b[0m)                                                     \n",
       "\n",
       " output               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49959\u001b[0m)               \u001b[38;5;34m0\u001b[0m  outputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n",
       " (\u001b[38;5;33mConcatenate\u001b[0m)                                       pi[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
       "                                                     dispersion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,141,639</span> (65.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,141,639\u001b[0m (65.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,141,639</span> (65.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,141,639\u001b[0m (65.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set clustering weights\n",
      "Done\n",
      "..Starting Deep Clustering\n",
      "Iter-0: CA=0.6019, NMI= 0.7095, ARI= 0.5403; L= 0.45336, Lc= 0.22525,  Lr= 0.22692\n",
      "CA=0.6643, val_NMI= 0.7460, val_ARI= 0.5814; val_L= 0.45227, val_Lc= 0.20632,  val_Lr= 0.24258\n",
      "WARNING:tensorflow:5 out of the last 452 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x147ab58abba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 453 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x147ab58abba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Iter-14: CA=0.5987, NMI= 0.7111, ARI= 0.5403; L= 0.44647, Lc= 0.21495,  Lr= 0.23087\n",
      "CA=0.6491, val_NMI= 0.7431, val_ARI= 0.5766; val_L= 0.43827, val_Lc= 0.19244,  val_Lr= 0.24116\n",
      "Iter-28: CA=0.5984, NMI= 0.7120, ARI= 0.5351; L= 0.42859, Lc= 0.19491,  Lr= 0.23371\n",
      "CA=0.6807, val_NMI= 0.7367, val_ARI= 0.6023; val_L= 0.43931, val_Lc= 0.19564,  val_Lr= 0.23948\n",
      "Iter-42: CA=0.5928, NMI= 0.7127, ARI= 0.5392; L= 0.42729, Lc= 0.18962,  Lr= 0.23822\n",
      "CA=0.6292, val_NMI= 0.7360, val_ARI= 0.5766; val_L= 0.44025, val_Lc= 0.18617,  val_Lr= 0.24985\n",
      "Iter-56: CA=0.5995, NMI= 0.7127, ARI= 0.5380; L= 0.46679, Lc= 0.22608,  Lr= 0.24042\n",
      "CA=0.7170, val_NMI= 0.7506, val_ARI= 0.6384; val_L= 0.43731, val_Lc= 0.19122,  val_Lr= 0.24456\n",
      "Iter-70: CA=0.6300, NMI= 0.7140, ARI= 0.5496; L= 0.43457, Lc= 0.19857,  Lr= 0.23686\n",
      "CA=0.7287, val_NMI= 0.7549, val_ARI= 0.6277; val_L= 0.43125, val_Lc= 0.18518,  val_Lr= 0.24159\n",
      "Iter-84: CA=0.5966, NMI= 0.7114, ARI= 0.5366; L= 0.43961, Lc= 0.20228,  Lr= 0.23755\n",
      "CA=0.6210, val_NMI= 0.7326, val_ARI= 0.5649; val_L= 0.43850, val_Lc= 0.18982,  val_Lr= 0.24395\n",
      "Iter-98: CA=0.6215, NMI= 0.7089, ARI= 0.5463; L= 0.42362, Lc= 0.18674,  Lr= 0.23777\n",
      "CA=0.7263, val_NMI= 0.7500, val_ARI= 0.6636; val_L= 0.42801, val_Lc= 0.18290,  val_Lr= 0.24187\n"
     ]
    }
   ],
   "source": [
    "for i in range (1):\n",
    "    y_pred, model=fit_and_split(data_ann.X, data_ann.obs,  data_ann.obs.size_factors, data_ann.raw.X, method=\"leiden\", res=10/10, alpha=1.0, noise=0.5, n_cluster=\"auto\", batch_size=256,\n",
    "             max_iter=2e3, tol=1e-3, update_interval=27, loss_weights=[1,1],\n",
    "             pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b7f2c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for res in range (9,11):\n",
    "#    for i in range(4):\n",
    "#        y_pred, model=fit_and_split(data_ann.X, data_ann.obs,  data_ann.obs.size_factors, data_ann.raw.X, method=\"leiden\", res=res/10, alpha=1.0, noise=0.5, n_cluster=\"auto\", batch_size=256,\n",
    "#             max_iter=2e3, tol=1e-3, update_interval=27, loss_weights=[1,1],\n",
    "#             pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c2d77-4708-4543-bbe4-89ef71337e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_scivar",
   "language": "python",
   "name": "env_scivar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
